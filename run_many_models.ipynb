{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a837d78-5dff-48d9-9753-37e382d9f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess\n",
      "MainProcess\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'flg_support' from 'd:\\\\flagellar/code/core\\\\flg_support.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('d:/flagellar/code/core')\n",
    "sys.path.append('/flagellar/code/core/')\n",
    "sys.path.append('/kaggle/input/my-flagellar-library/')\n",
    "import flg_support as fls\n",
    "import flg_runner\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "fast_mode = (fls.env=='local')\n",
    "importlib.reload(fls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee156f6e-b2e1-423b-becd-381c3151b98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "if not fast_mode:\n",
    "    fls.download_kaggle_dataset('jeroencottaar/byu-many-models/', fls.result_dir + '/many_full/')\n",
    "    fls.download_kaggle_dataset('jeroencottaar/byu-many-models-abbreviated/', fls.result_dir + '/many_abbr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c70ac9-1bc6-4ffe-a3aa-cf2d6cbeba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 200\n",
      "10 5\n",
      "{'seed': 1, 'scale_percentile_value': 3.047286498801027, 'img_size': 576, 'model_name': 'yolo11m', 'use_pretrained_weights': False, 'box_size': 29, 'trust': 5, 'fix_norm_bug': False, 'weight_decay': 0.00042332644897257566, 'hsv_h': 0.015, 'hsv_s': 0.0, 'hsv_v': 0.0, 'translate': 0.0, 'scale': 0.5274591760723646, 'fliplr': 0.5, 'flipud': 0.5, 'degrees': 0, 'shear': 0.0, 'mosaic': 0.0, 'mixup': 0.0, 'erasing': 0.0, 'use_albumentations': False, 'confidence_threshold': 0.5000729345260105}\n",
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 0.43698 s\n",
      "File: d:\\flagellar/code/core\\flg_preprocess.py\n",
      "Function: load_and_preprocess at line 40\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    40                                               @fls.profile_each_line\n",
      "    41                                               def load_and_preprocess(self, data, desired_original_slices = None):\n",
      "    42                                           \n",
      "    43         1    2505326.0    3e+06     57.3          data.load_to_memory(desired_slices = desired_original_slices, pad_to_original_size = self.pad_to_original_size)\n",
      "    44                                           \n",
      "    45         1         75.0     75.0      0.0          fls.claim_gpu('cupy')\n",
      "    46         1    1336869.0    1e+06     30.6          img = cp.array(data.data).astype(cp.float16)\n",
      "    47                                           \n",
      "    48                                                   # Resize\n",
      "    49         1         24.0     24.0      0.0          if self.resize:\n",
      "    50                                                       print('reconsider')\n",
      "    51                                                       import cupyx.scipy.ndimage\n",
      "    52                                                       #print(img.shape)\n",
      "    53                                                       data.resize_factor = min(self.resize_target/img.shape[1], self.resize_target/img.shape[2])\n",
      "    54                                                       #test_data = cupyx.scipy.ndimage.zoom(img[0,:,:], data.resize_factor)\n",
      "    55                                                       # data_new = cp.zeros((img.shape[0], test_data.shape[0], test_data.shape[1]), dtype=img.dtype)\n",
      "    56                                                       # for ii in range(img.shape[0]):\n",
      "    57                                                       #     data_new[ii,:,:] = cupyx.scipy.ndimage.zoom(img[ii,:,:], data.resize_factor)\n",
      "    58                                                       # img = data_new\n",
      "    59                                                       # test_data = cupyx.scipy.ndimage.zoom(img[:,0,0], data.resize_factor)\n",
      "    60                                                       # data_new = cp.zeros((test_data.shape[0], img.shape[1], img.shape[2]), dtype=img.dtype)\n",
      "    61                                                       # for ii in range(img.shape[1]):\n",
      "    62                                                       #     data_new[:,ii,:] = cupyx.scipy.ndimage.zoom(img[:,ii,:], (data.resize_factor,1.))\n",
      "    63                                                       img = cupyx.scipy.ndimage.zoom(img, (data.resize_factor,data.resize_factor,data.resize_factor))\n",
      "    64                                                       #print(img.shape)\n",
      "    65                                           \n",
      "    66                                                       data.data_shape = img.shape\n",
      "    67                                                       data.voxel_spacing = data.voxel_spacing/data.resize_factor\n",
      "    68                                                   else:\n",
      "    69         1         86.0     86.0      0.0              data.resize_factor = 1.\n",
      "    70                                           \n",
      "    71                                                   # Scale percentile\n",
      "    72         1          6.0      6.0      0.0          if self.scale_percentile:\n",
      "    73                                                       for ii in range(img.shape[0]):\n",
      "    74                                                           perc_low = cp.percentile(img[ii,:,:], self.scale_percentile_value)\n",
      "    75                                                           perc_high = cp.percentile(img[ii,:,:], 100-self.scale_percentile_value)\n",
      "    76                                                           img[ii,:,:] = (img[ii,:,:]-perc_low)/(perc_high-perc_low)\n",
      "    77                                                       if self.scale_percentile_clip:\n",
      "    78                                                           img[img>1.] = 1.\n",
      "    79                                                           img[img<0.] = 0.\n",
      "    80                                           \n",
      "    81                                                   # Scale STD\n",
      "    82         1          6.0      6.0      0.0          if self.scale_std:\n",
      "    83                                                       mean_per_slice = cp.mean(img,axis=(1,2))\n",
      "    84                                                       std_per_slice = cp.std(img.astype(cp.float32),axis=(1,2)).astype(cp.float16)\n",
      "    85                                                       img = (img - mean_per_slice[:,None,None]) / std_per_slice[:,None,None]\n",
      "    86                                                       #for ii in range(img.shape[0]):\n",
      "    87                                                       #    img[ii,:,:,] = (img[ii,:,:,]-mean_list[ii])/std_list[ii]\n",
      "    88                                                   \n",
      "    89                                                   # Cast to uint8\n",
      "    90         1          5.0      5.0      0.0          if self.return_uint8:\n",
      "    91         1          2.0      2.0      0.0              if self.scale_percentile:\n",
      "    92                                                           img = (255*img).astype(cp.uint8)\n",
      "    93                                                       else:\n",
      "    94         1      19946.0  19946.0      0.5                  img = (img).astype(cp.uint8)\n",
      "    95         1          9.0      9.0      0.0              assert not self.scale_std\n",
      "    96                                           \n",
      "    97         1     507447.0 507447.0     11.6          data.data = cp.asnumpy(img)\n",
      "\n",
      "Clearing cupy\n",
      "YOLO11m summary (fused): 125 layers, 20,030,803 parameters, 0 gradients, 67.6 GFLOPs\n",
      "Processing tomogram tomo_0363f2 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "Motor found in tomo_0363f2 at position: z=-1, y=-1, x=-1\n",
      "tomo_0363f2 total infer time: 10.096513986587524\n",
      "Clearing pytorch\n",
      "Timer unit: 1e-07 s\n",
      "\n",
      "Total time: 1.29976 s\n",
      "File: d:\\flagellar/code/core\\flg_preprocess.py\n",
      "Function: load_and_preprocess at line 40\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    40                                               @fls.profile_each_line\n",
      "    41                                               def load_and_preprocess(self, data, desired_original_slices = None):\n",
      "    42                                           \n",
      "    43         1    7201106.0    7e+06     55.4          data.load_to_memory(desired_slices = desired_original_slices, pad_to_original_size = self.pad_to_original_size)\n",
      "    44                                           \n",
      "    45         1    1376046.0    1e+06     10.6          fls.claim_gpu('cupy')\n",
      "    46         1    1921411.0    2e+06     14.8          img = cp.array(data.data).astype(cp.float16)\n",
      "    47                                           \n",
      "    48                                                   # Resize\n",
      "    49         1         17.0     17.0      0.0          if self.resize:\n",
      "    50                                                       print('reconsider')\n",
      "    51                                                       import cupyx.scipy.ndimage\n",
      "    52                                                       #print(img.shape)\n",
      "    53                                                       data.resize_factor = min(self.resize_target/img.shape[1], self.resize_target/img.shape[2])\n",
      "    54                                                       #test_data = cupyx.scipy.ndimage.zoom(img[0,:,:], data.resize_factor)\n",
      "    55                                                       # data_new = cp.zeros((img.shape[0], test_data.shape[0], test_data.shape[1]), dtype=img.dtype)\n",
      "    56                                                       # for ii in range(img.shape[0]):\n",
      "    57                                                       #     data_new[ii,:,:] = cupyx.scipy.ndimage.zoom(img[ii,:,:], data.resize_factor)\n",
      "    58                                                       # img = data_new\n",
      "    59                                                       # test_data = cupyx.scipy.ndimage.zoom(img[:,0,0], data.resize_factor)\n",
      "    60                                                       # data_new = cp.zeros((test_data.shape[0], img.shape[1], img.shape[2]), dtype=img.dtype)\n",
      "    61                                                       # for ii in range(img.shape[1]):\n",
      "    62                                                       #     data_new[:,ii,:] = cupyx.scipy.ndimage.zoom(img[:,ii,:], (data.resize_factor,1.))\n",
      "    63                                                       img = cupyx.scipy.ndimage.zoom(img, (data.resize_factor,data.resize_factor,data.resize_factor))\n",
      "    64                                                       #print(img.shape)\n",
      "    65                                           \n",
      "    66                                                       data.data_shape = img.shape\n",
      "    67                                                       data.voxel_spacing = data.voxel_spacing/data.resize_factor\n",
      "    68                                                   else:\n",
      "    69         1         86.0     86.0      0.0              data.resize_factor = 1.\n",
      "    70                                           \n",
      "    71                                                   # Scale percentile\n",
      "    72         1          7.0      7.0      0.0          if self.scale_percentile:\n",
      "    73                                                       for ii in range(img.shape[0]):\n",
      "    74                                                           perc_low = cp.percentile(img[ii,:,:], self.scale_percentile_value)\n",
      "    75                                                           perc_high = cp.percentile(img[ii,:,:], 100-self.scale_percentile_value)\n",
      "    76                                                           img[ii,:,:] = (img[ii,:,:]-perc_low)/(perc_high-perc_low)\n",
      "    77                                                       if self.scale_percentile_clip:\n",
      "    78                                                           img[img>1.] = 1.\n",
      "    79                                                           img[img<0.] = 0.\n",
      "    80                                           \n",
      "    81                                                   # Scale STD\n",
      "    82         1          4.0      4.0      0.0          if self.scale_std:\n",
      "    83                                                       mean_per_slice = cp.mean(img,axis=(1,2))\n",
      "    84                                                       std_per_slice = cp.std(img.astype(cp.float32),axis=(1,2)).astype(cp.float16)\n",
      "    85                                                       img = (img - mean_per_slice[:,None,None]) / std_per_slice[:,None,None]\n",
      "    86                                                       #for ii in range(img.shape[0]):\n",
      "    87                                                       #    img[ii,:,:,] = (img[ii,:,:,]-mean_list[ii])/std_list[ii]\n",
      "    88                                                   \n",
      "    89                                                   # Cast to uint8\n",
      "    90         1          7.0      7.0      0.0          if self.return_uint8:\n",
      "    91         1          3.0      3.0      0.0              if self.scale_percentile:\n",
      "    92                                                           img = (255*img).astype(cp.uint8)\n",
      "    93                                                       else:\n",
      "    94         1        339.0    339.0      0.0                  img = (img).astype(cp.uint8)\n",
      "    95         1          2.0      2.0      0.0              assert not self.scale_std\n",
      "    96                                           \n",
      "    97         1    2498584.0    2e+06     19.2          data.data = cp.asnumpy(img)\n",
      "\n",
      "Clearing cupy\n",
      "Processing tomogram tomo_05b39c (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n"
     ]
    }
   ],
   "source": [
    "#fls.profiling=True\n",
    "importlib.reload(flg_runner)\n",
    "import git \n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "git_commit_id = repo.head.object.hexsha\n",
    "for i in itertools.count():\n",
    "    r = flg_runner.baseline_runner(fast_mode=fast_mode)\n",
    "    r.git_commit_id = git_commit_id\n",
    "    r.seed = i\n",
    "    #r.base_model.run_in_parallel = False\n",
    "    base_filename = r.label + '_' + str(r.seed) + '_' + git_commit_id[:8]\n",
    "    output_file_full = fls.result_dir + '/many_full/' + base_filename + '_f.pickle'\n",
    "    output_file_abbr = fls.result_dir + '/many_abbr/' + base_filename + '_a.pickle'\n",
    "    if os.path.isfile(output_file_full):\n",
    "        continue\n",
    "    r.run()\n",
    "    fls.dill_save(output_file_full, r)\n",
    "    r.trained_model = 0\n",
    "    fls.dill_save(output_file_abbr, r)\n",
    "    if not fast_mode:\n",
    "        fls.upload_kaggle_dataset(fls.result_dir + '/many_full/')\n",
    "        fls.upload_kaggle_dataset(fls.result_dir + '/many_abbr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1132e155-7926-4b92-b859-11f63079b829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "isSourceIdPinned": false,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 6925042,
     "sourceId": 11204341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6949538,
     "sourceId": 11204343,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211097053,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 229283084,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
