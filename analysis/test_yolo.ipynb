{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328447bf-3047-4562-bdf6-60dfc83bae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#    raise 'stop'\n",
    "if not os.path.isdir('d:/flagellar/'):\n",
    "    deps_path = '/kaggle/usr/lib/flg_packages/'\n",
    "    !pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt\n",
    "    !tar xfvz /kaggle/input/ultralytics-for-offline-install/archive.tar.gz\n",
    "    !pip install --no-index --find-links=./packages ultralytics\n",
    "    !rm -rf ./packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a837d78-5dff-48d9-9753-37e382d9f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import sys\n",
    "sys.path.append('d:/flagellar/code/core')\n",
    "sys.path.append('/flagellar/code/core/')\n",
    "sys.path.append('/kaggle/input/my-flagellar-library/')\n",
    "import flg_support as fls\n",
    "import flg_unet\n",
    "import flg_numerics\n",
    "import flg_model\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import flg_yolo\n",
    "\n",
    "fast_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26398b77-77a4-4d7c-804c-79950c1aae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = fls.load_all_train_data()\n",
    "np.random.default_rng(seed=0).shuffle(all_data)\n",
    "\n",
    "# Pick N tomograms with 1 motor and N tomograms with 0 motors as validation set\n",
    "N=100\n",
    "n_motors = np.array([len(d.labels) for d in all_data])\n",
    "inds_zero = np.argwhere(n_motors==0)[:N,0]\n",
    "inds_one = np.argwhere(n_motors==1)[:N,0]\n",
    "inds_test = np.concatenate((inds_zero,inds_one))\n",
    "inds_train = np.setdiff1d(np.arange(len(n_motors)), inds_test)\n",
    "inds_test.shape, inds_train.shape\n",
    "\n",
    "train_data = []\n",
    "for i in inds_train:\n",
    "    train_data.append(all_data[i])\n",
    "test_data = []\n",
    "for i in inds_test:\n",
    "    test_data.append(all_data[i])\n",
    "np.random.default_rng(seed=0).shuffle(test_data)\n",
    "test_data = test_data\n",
    "if fast_mode:\n",
    "    train_data = train_data[1:30]\n",
    "    test_data = test_data[2:4]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c805e897-9b02-4264-bcbf-4478b57ca9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 3123 slices for training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2543fa27f4074aad81eb487ecb309e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing training motors:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will process approximately 900 slices for validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e498b492dc49489ef8389f13d2fc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing validation motors:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Summary:\n",
      "- Train set: 258 tomograms, 347 motors, 3118 slices\n",
      "- Validation set: 100 tomograms, 100 motors, 900 slices\n",
      "- Total: 358 tomograms, 447 motors, 4018 slices\n",
      "\n",
      "Preprocessing Complete:\n",
      "- Training data: 258 tomograms, 347 motors, 3118 slices\n",
      "- Validation data: 100 tomograms, 100 motors, 900 slices\n",
      "- Dataset directory: d:/flagellar/temp//yolo_dataset/\n",
      "- YAML configuration: d:/flagellar/temp//yolo_dataset/dataset.yaml\n",
      "\n",
      "Ready for YOLO training!\n",
      "Starting YOLO training process...\n",
      "Directory status:\n",
      "- Train images exists: True\n",
      "- Val images exists: True\n",
      "- Train labels exists: True\n",
      "- Val labels exists: True\n",
      "Found original dataset.yaml at d:/flagellar/temp//yolo_dataset/dataset.yaml\n",
      "Fixing YAML paths in d:/flagellar/temp//yolo_dataset/dataset.yaml\n",
      "Created fixed YAML at d:/flagellar/temp/fixed_dataset.yaml with path: d:/flagellar/temp//yolo_dataset/\n",
      "Using YAML file: d:/flagellar/temp/fixed_dataset.yaml\n",
      "YAML contents:\n",
      "names:\n",
      "  0: motor\n",
      "path: d:/flagellar/temp//yolo_dataset/\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n",
      "\n",
      "Starting YOLO training...\n",
      "Loading pre-trained weights from: d:/flagellar/models//yolov8m.pt\n",
      "Ultralytics 8.3.98  Python-3.10.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12282MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=d:/flagellar/models//yolov8m.pt, data=d:/flagellar/temp/fixed_dataset.yaml, epochs=30, time=None, patience=10, batch=16, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=4, project=d:/flagellar/temp//yolo_weights/, name=motor_detector, exist_ok=True, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.2, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=d:\\flagellar\\temp\\yolo_weights\\motor_detector\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3776275  ultralytics.nn.modules.head.Detect           [1, [192, 384, 576]]          \n",
      "Model summary: 169 layers, 25,856,899 parameters, 25,856,883 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[34m\u001b[1mtrain: \u001b[0mScanning D:\\flagellar\\temp\\yolo_dataset\\labels\\train... 3118 images, 0 backgrounds, 0 corrupt: 100%|██████████| \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: D:\\flagellar\\temp\\yolo_dataset\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[34m\u001b[1mval: \u001b[0mScanning D:\\flagellar\\temp\\yolo_dataset\\labels\\val... 900 images, 0 backgrounds, 0 corrupt: 100%|██████████| 900/9\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: D:\\flagellar\\temp\\yolo_dataset\\labels\\val.cache\n",
      "Plotting labels to d:\\flagellar\\temp\\yolo_weights\\motor_detector\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/29 10:30:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/03/29 10:30:05 WARNING mlflow.utils.autologging_utils: MLflow transformers autologging is known to be compatible with 4.25.1 <= transformers <= 4.46.3, but the installed version is 4.48.3. If you encounter errors during autologging, try upgrading / downgrading transformers to a compatible version, or try upgrading MLflow.\n",
      "2025/03/29 10:30:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(caf18c5835b44f999199c08dfa51e009) to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs\\mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1md:\\flagellar\\temp\\yolo_weights\\motor_detector\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.417      0.509      0.392      0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.522      0.492      0.457      0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.651      0.456      0.498      0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.635      0.646      0.577      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.809      0.701      0.728      0.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.687      0.597      0.609      0.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.576      0.449       0.43     0.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.875       0.78       0.82      0.349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.781      0.672      0.685      0.209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.877      0.828      0.861      0.317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.919      0.847      0.908      0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900       0.78       0.72      0.714      0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.898      0.841      0.902      0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.941      0.828      0.911       0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.921      0.869       0.92      0.411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.927      0.863       0.91      0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.929      0.868      0.916      0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900       0.91       0.82      0.873       0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.912       0.83      0.875      0.367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.903      0.821      0.873      0.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:07"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.915      0.845      0.891      0.344\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 10 epochs. Best results observed at epoch 11, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21 epochs completed in 0.305 hours.\n",
      "Optimizer stripped from d:\\flagellar\\temp\\yolo_weights\\motor_detector\\weights\\last.pt, 52.0MB\n",
      "Optimizer stripped from d:\\flagellar\\temp\\yolo_weights\\motor_detector\\weights\\best.pt, 52.0MB\n",
      "\n",
      "Validating d:\\flagellar\\temp\\yolo_weights\\motor_detector\\weights\\best.pt...\n",
      "Ultralytics 8.3.98  Python-3.10.14 torch-2.5.1 CUDA:0 (NVIDIA GeForce RTX 4070 Ti, 12282MiB)\n",
      "Model summary (fused): 92 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 29/29 [00:11"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        900        900      0.694       0.83      0.793       0.36\n",
      "Speed: 0.1ms preprocess, 7.8ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1md:\\flagellar\\temp\\yolo_weights\\motor_detector\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(flg_yolo)\n",
    "model = flg_yolo.YOLOModel()\n",
    "model.seed = 42\n",
    "model.run_in_parallel = True\n",
    "if fast_mode: model.n_epochs = 1\n",
    "model.train(train_data, test_data)\n",
    "fls.dill_save(fls.temp_dir+ \"temp.pickle\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325679ee-dd1b-4fdf-8414-d2520638f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.84GB free memory\n",
      "Model summary (fused): 92 layers, 25,840,339 parameters, 0 gradients, 78.7 GFLOPs\n",
      "Processing tomogram d:/flagellar/data//train/tomo_ff505c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.804s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 2.221s\n",
      "[PROFILE] Inference batch 2/4: 0.043s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.045s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_ff505c', 'Motor axis 0': 111, 'Motor axis 1': 816, 'Motor axis 2': 679}\n",
      "Motor found in d:/flagellar/data//train/tomo_ff505c at position: z=111, y=816, x=679\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_6c4df3 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.534s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 2.361s\n",
      "[PROFILE] Inference batch 2/4: 0.068s\n",
      "[PROFILE] Inference batch 3/4: 0.092s\n",
      "[PROFILE] Inference batch 4/4: 0.072s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_6c4df3', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_6c4df3 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_62dbea (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.511s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.122s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 2.154s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_62dbea', 'Motor axis 0': 85, 'Motor axis 1': 656, 'Motor axis 2': 782}\n",
      "Motor found in d:/flagellar/data//train/tomo_62dbea at position: z=85, y=656, x=782\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_fbb49b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.508s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 2.224s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.043s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_fbb49b', 'Motor axis 0': 141, 'Motor axis 1': 402, 'Motor axis 2': 355}\n",
      "Motor found in d:/flagellar/data//train/tomo_fbb49b at position: z=141, y=402, x=355\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_71d2c0 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.517s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_71d2c0', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_71d2c0 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_6733fa (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.632s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 2.153s\n",
      "[PROFILE] Inference batch 2/4: 0.054s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.046s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_6733fa', 'Motor axis 0': 135, 'Motor axis 1': 325, 'Motor axis 2': 750}\n",
      "Motor found in d:/flagellar/data//train/tomo_6733fa at position: z=135, y=325, x=750\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c4db00 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.496s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.124s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 2.170s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c4db00', 'Motor axis 0': 127, 'Motor axis 1': 149, 'Motor axis 2': 573}\n",
      "Motor found in d:/flagellar/data//train/tomo_c4db00 at position: z=127, y=149, x=573\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_9fc2b6 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.524s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.360s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.047s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_9fc2b6', 'Motor axis 0': 131, 'Motor axis 1': 727, 'Motor axis 2': 568}\n",
      "Motor found in d:/flagellar/data//train/tomo_9fc2b6 at position: z=131, y=727, x=568\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_13484c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.549s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.108s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 2.174s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_13484c', 'Motor axis 0': 151, 'Motor axis 1': 767, 'Motor axis 2': 660}\n",
      "Motor found in d:/flagellar/data//train/tomo_13484c at position: z=151, y=767, x=660\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_622ca9 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.516s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 2.159s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.047s\n",
      "[PROFILE] Inference batch 4/4: 0.052s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_622ca9', 'Motor axis 0': 89, 'Motor axis 1': 344, 'Motor axis 2': 566}\n",
      "Motor found in d:/flagellar/data//train/tomo_622ca9 at position: z=89, y=344, x=566\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_651ec2 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.486s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 2.332s\n",
      "[PROFILE] Inference batch 2/4: 0.070s\n",
      "[PROFILE] Inference batch 3/4: 0.095s\n",
      "[PROFILE] Inference batch 4/4: 0.074s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_651ec2', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_651ec2 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_78b03d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.529s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.134s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 2.166s\n",
      "[PROFILE] Inference batch 2/4: 0.054s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_78b03d', 'Motor axis 0': 103, 'Motor axis 1': 587, 'Motor axis 2': 430}\n",
      "Motor found in d:/flagellar/data//train/tomo_78b03d at position: z=103, y=587, x=430\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_319f79 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.498s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.126s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.141s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 2.344s\n",
      "[PROFILE] Inference batch 2/4: 0.070s\n",
      "[PROFILE] Inference batch 3/4: 0.095s\n",
      "[PROFILE] Inference batch 4/4: 0.095s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_319f79', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_319f79 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_518a1f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.601s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.142s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 2.215s\n",
      "[PROFILE] Inference batch 2/4: 0.055s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_518a1f', 'Motor axis 0': 113, 'Motor axis 1': 564, 'Motor axis 2': 679}\n",
      "Motor found in d:/flagellar/data//train/tomo_518a1f at position: z=113, y=564, x=679\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cc65a9 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.596s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.101s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.124s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 2.191s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cc65a9', 'Motor axis 0': 145, 'Motor axis 1': 264, 'Motor axis 2': 483}\n",
      "Motor found in d:/flagellar/data//train/tomo_cc65a9 at position: z=145, y=264, x=483\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dbc66d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.504s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.134s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.127s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 2.168s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dbc66d', 'Motor axis 0': 165, 'Motor axis 1': 377, 'Motor axis 2': 403}\n",
      "Motor found in d:/flagellar/data//train/tomo_dbc66d at position: z=165, y=377, x=403\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_399bd9 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.476s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.144s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 2.182s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.051s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_399bd9', 'Motor axis 0': 129, 'Motor axis 1': 510, 'Motor axis 2': 364}\n",
      "Motor found in d:/flagellar/data//train/tomo_399bd9 at position: z=129, y=510, x=364\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_94c173 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.507s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.141s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.172s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.044s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_94c173', 'Motor axis 0': 55, 'Motor axis 1': 542, 'Motor axis 2': 367}\n",
      "Motor found in d:/flagellar/data//train/tomo_94c173 at position: z=55, y=542, x=367\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f1bf2f (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.486s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.121s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.148s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.108s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.107s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.136s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.117s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f1bf2f', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_f1bf2f at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_19a4fd (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.447s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.125s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 2.168s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_19a4fd', 'Motor axis 0': 172, 'Motor axis 1': 582, 'Motor axis 2': 465}\n",
      "Motor found in d:/flagellar/data//train/tomo_19a4fd at position: z=172, y=582, x=465\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_d23087 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.501s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.126s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 2.169s\n",
      "[PROFILE] Inference batch 2/4: 0.044s\n",
      "[PROFILE] Inference batch 3/4: 0.046s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_d23087', 'Motor axis 0': 83, 'Motor axis 1': 474, 'Motor axis 2': 355}\n",
      "Motor found in d:/flagellar/data//train/tomo_d23087 at position: z=83, y=474, x=355\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_161683 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.507s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 2.248s\n",
      "[PROFILE] Inference batch 2/4: 0.056s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_161683', 'Motor axis 0': 190, 'Motor axis 1': 142, 'Motor axis 2': 370}\n",
      "Motor found in d:/flagellar/data//train/tomo_161683 at position: z=190, y=142, x=370\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_9cd09e (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.687s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 2.187s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.044s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_9cd09e', 'Motor axis 0': 124, 'Motor axis 1': 816, 'Motor axis 2': 770}\n",
      "Motor found in d:/flagellar/data//train/tomo_9cd09e at position: z=124, y=816, x=770\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_6b1fd3 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.527s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 2.307s\n",
      "[PROFILE] Inference batch 2/4: 0.087s\n",
      "[PROFILE] Inference batch 3/4: 0.071s\n",
      "[PROFILE] Inference batch 4/4: 0.096s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_6b1fd3', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_6b1fd3 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c00ab5 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.473s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.108s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 2.164s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c00ab5', 'Motor axis 0': 108, 'Motor axis 1': 529, 'Motor axis 2': 463}\n",
      "Motor found in d:/flagellar/data//train/tomo_c00ab5 at position: z=108, y=529, x=463\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_285454 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.444s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 2.194s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.054s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_285454', 'Motor axis 0': 183, 'Motor axis 1': 865, 'Motor axis 2': 517}\n",
      "Motor found in d:/flagellar/data//train/tomo_285454 at position: z=183, y=865, x=517\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cf0875 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.523s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 2.178s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cf0875', 'Motor axis 0': 90, 'Motor axis 1': 604, 'Motor axis 2': 646}\n",
      "Motor found in d:/flagellar/data//train/tomo_cf0875 at position: z=90, y=604, x=646\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_8f063a (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.504s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 2.168s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_8f063a', 'Motor axis 0': 155, 'Motor axis 1': 384, 'Motor axis 2': 96}\n",
      "Motor found in d:/flagellar/data//train/tomo_8f063a at position: z=155, y=384, x=96\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_6c203d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.504s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.110s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.113s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 2.156s\n",
      "[PROFILE] Inference batch 2/4: 0.052s\n",
      "[PROFILE] Inference batch 3/4: 0.054s\n",
      "[PROFILE] Inference batch 4/4: 0.053s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_6c203d', 'Motor axis 0': 168, 'Motor axis 1': 310, 'Motor axis 2': 277}\n",
      "Motor found in d:/flagellar/data//train/tomo_6c203d at position: z=168, y=310, x=277\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_516cdd (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.528s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 2.176s\n",
      "[PROFILE] Inference batch 2/4: 0.045s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_516cdd', 'Motor axis 0': 184, 'Motor axis 1': 737, 'Motor axis 2': 180}\n",
      "Motor found in d:/flagellar/data//train/tomo_516cdd at position: z=184, y=737, x=180\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dee783 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.544s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.204s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 2.175s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.044s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dee783', 'Motor axis 0': 130, 'Motor axis 1': 519, 'Motor axis 2': 624}\n",
      "Motor found in d:/flagellar/data//train/tomo_dee783 at position: z=130, y=519, x=624\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_ca8be0 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.611s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.292s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 2.342s\n",
      "[PROFILE] Inference batch 2/4: 0.067s\n",
      "[PROFILE] Inference batch 3/4: 0.080s\n",
      "[PROFILE] Inference batch 4/4: 0.081s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_ca8be0', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_ca8be0 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_16efa8 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.480s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.127s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.106s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.125s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 2.155s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.053s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_16efa8', 'Motor axis 0': 184, 'Motor axis 1': 379, 'Motor axis 2': 751}\n",
      "Motor found in d:/flagellar/data//train/tomo_16efa8 at position: z=184, y=379, x=751\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_556257 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.475s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.140s\n",
      "[PROFILE] Inference batch 2/4: 0.104s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 2.160s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_556257', 'Motor axis 0': 132, 'Motor axis 1': 206, 'Motor axis 2': 429}\n",
      "Motor found in d:/flagellar/data//train/tomo_556257 at position: z=132, y=206, x=429\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cd1a7c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.458s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.136s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 2.149s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.051s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cd1a7c', 'Motor axis 0': 165, 'Motor axis 1': 485, 'Motor axis 2': 701}\n",
      "Motor found in d:/flagellar/data//train/tomo_cd1a7c at position: z=165, y=485, x=701\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_16fce8 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.475s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.138s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 2.137s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_16fce8', 'Motor axis 0': 194, 'Motor axis 1': 533, 'Motor axis 2': 394}\n",
      "Motor found in d:/flagellar/data//train/tomo_16fce8 at position: z=194, y=533, x=394\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_401341 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.510s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.134s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.108s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 2.172s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_401341', 'Motor axis 0': 161, 'Motor axis 1': 613, 'Motor axis 2': 722}\n",
      "Motor found in d:/flagellar/data//train/tomo_401341 at position: z=161, y=613, x=722\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_72b187 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.514s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 2.189s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_72b187', 'Motor axis 0': 97, 'Motor axis 1': 851, 'Motor axis 2': 121}\n",
      "Motor found in d:/flagellar/data//train/tomo_72b187 at position: z=97, y=851, x=121\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_997437 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.452s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 2.162s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_997437', 'Motor axis 0': 122, 'Motor axis 1': 756, 'Motor axis 2': 765}\n",
      "Motor found in d:/flagellar/data//train/tomo_997437 at position: z=122, y=756, x=765\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_0c2749 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.454s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.106s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.134s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 2.150s\n",
      "[PROFILE] Inference batch 2/4: 0.056s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_0c2749', 'Motor axis 0': 147, 'Motor axis 1': 607, 'Motor axis 2': 446}\n",
      "Motor found in d:/flagellar/data//train/tomo_0c2749 at position: z=147, y=607, x=446\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_db656f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.589s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 2.185s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_db656f', 'Motor axis 0': 133, 'Motor axis 1': 280, 'Motor axis 2': 561}\n",
      "Motor found in d:/flagellar/data//train/tomo_db656f at position: z=133, y=280, x=561\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_5b359d (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.487s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.103s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.099s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.099s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.137s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.136s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.146s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.145s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.111s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.211s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.106s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_5b359d', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_5b359d at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_bb5ac1 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.489s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 2.178s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.038s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_bb5ac1', 'Motor axis 0': 134, 'Motor axis 1': 603, 'Motor axis 2': 485}\n",
      "Motor found in d:/flagellar/data//train/tomo_bb5ac1 at position: z=134, y=603, x=485\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_643b20 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.496s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.106s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.125s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 2.216s\n",
      "[PROFILE] Inference batch 2/4: 0.044s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_643b20', 'Motor axis 0': 162, 'Motor axis 1': 385, 'Motor axis 2': 591}\n",
      "Motor found in d:/flagellar/data//train/tomo_643b20 at position: z=162, y=385, x=591\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_22976c (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.511s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.138s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 2.277s\n",
      "[PROFILE] Inference batch 2/4: 0.089s\n",
      "[PROFILE] Inference batch 3/4: 0.086s\n",
      "[PROFILE] Inference batch 4/4: 0.080s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_22976c', 'Motor axis 0': 249, 'Motor axis 1': 115, 'Motor axis 2': 114}\n",
      "Motor found in d:/flagellar/data//train/tomo_22976c at position: z=249, y=115, x=114\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dae195 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.504s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.105s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dae195', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_dae195 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_df866a (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.469s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.121s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 2.137s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_df866a', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_df866a at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_30b580 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.635s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.101s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.217s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 2.160s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_30b580', 'Motor axis 0': 147, 'Motor axis 1': 396, 'Motor axis 2': 570}\n",
      "Motor found in d:/flagellar/data//train/tomo_30b580 at position: z=147, y=396, x=570\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_82d780 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.488s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.107s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.173s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_82d780', 'Motor axis 0': 108, 'Motor axis 1': 367, 'Motor axis 2': 574}\n",
      "Motor found in d:/flagellar/data//train/tomo_82d780 at position: z=108, y=367, x=574\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_47d380 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.517s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.125s\n",
      "[PROFILE] Inference batch 3/4: 0.110s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_47d380', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_47d380 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_95c0eb (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.451s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.115s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_95c0eb', 'Motor axis 0': 386, 'Motor axis 1': 420, 'Motor axis 2': 911}\n",
      "Motor found in d:/flagellar/data//train/tomo_95c0eb at position: z=386, y=420, x=911\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_4ed9de (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.473s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 2.251s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.048s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_4ed9de', 'Motor axis 0': 169, 'Motor axis 1': 725, 'Motor axis 2': 718}\n",
      "Motor found in d:/flagellar/data//train/tomo_4ed9de at position: z=169, y=725, x=718\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_ec1314 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.661s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.138s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 2.152s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_ec1314', 'Motor axis 0': 152, 'Motor axis 1': 584, 'Motor axis 2': 858}\n",
      "Motor found in d:/flagellar/data//train/tomo_ec1314 at position: z=152, y=584, x=858\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_134bb0 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.555s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.131s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 2.155s\n",
      "[PROFILE] Inference batch 2/4: 0.044s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_134bb0', 'Motor axis 0': 52, 'Motor axis 1': 253, 'Motor axis 2': 175}\n",
      "Motor found in d:/flagellar/data//train/tomo_134bb0 at position: z=52, y=253, x=175\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dd36c9 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.564s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.136s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.211s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.183s\n",
      "[PROFILE] Inference batch 1/4: 2.342s\n",
      "[PROFILE] Inference batch 2/4: 0.070s\n",
      "[PROFILE] Inference batch 3/4: 0.096s\n",
      "[PROFILE] Inference batch 4/4: 0.069s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dd36c9', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_dd36c9 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_aff073 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.593s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.117s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 2.517s\n",
      "[PROFILE] Inference batch 2/4: 0.074s\n",
      "[PROFILE] Inference batch 3/4: 0.095s\n",
      "[PROFILE] Inference batch 4/4: 0.083s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_aff073', 'Motor axis 0': 206, 'Motor axis 1': 65, 'Motor axis 2': 52}\n",
      "Motor found in d:/flagellar/data//train/tomo_aff073 at position: z=206, y=65, x=52\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cacb75 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.472s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.109s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.106s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 2.140s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.057s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cacb75', 'Motor axis 0': 242, 'Motor axis 1': 153, 'Motor axis 2': 505}\n",
      "Motor found in d:/flagellar/data//train/tomo_cacb75 at position: z=242, y=153, x=505\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_4077d8 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.469s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 2.168s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.078s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_4077d8', 'Motor axis 0': 69, 'Motor axis 1': 96, 'Motor axis 2': 225}\n",
      "Motor found in d:/flagellar/data//train/tomo_4077d8 at position: z=69, y=96, x=225\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_24fda8 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.483s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.142s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.126s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 2.168s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_24fda8', 'Motor axis 0': 182, 'Motor axis 1': 82, 'Motor axis 2': 537}\n",
      "Motor found in d:/flagellar/data//train/tomo_24fda8 at position: z=182, y=82, x=537\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a020d7 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.457s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.110s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.118s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 2.142s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.037s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a020d7', 'Motor axis 0': 197, 'Motor axis 1': 438, 'Motor axis 2': 318}\n",
      "Motor found in d:/flagellar/data//train/tomo_a020d7 at position: z=197, y=438, x=318\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_1c38fd (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.428s\n",
      "[PROFILE] Inference batch 2/4: 0.138s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 2.209s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_1c38fd', 'Motor axis 0': 102, 'Motor axis 1': 671, 'Motor axis 2': 282}\n",
      "Motor found in d:/flagellar/data//train/tomo_1c38fd at position: z=102, y=671, x=282\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dfdc32 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.420s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 2.286s\n",
      "[PROFILE] Inference batch 2/4: 0.076s\n",
      "[PROFILE] Inference batch 3/4: 0.084s\n",
      "[PROFILE] Inference batch 4/4: 0.088s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dfdc32', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_dfdc32 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_89d156 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.419s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.136s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.140s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.098s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.099s\n",
      "[PROFILE] Inference batch 1/4: 2.185s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_89d156', 'Motor axis 0': 139, 'Motor axis 1': 407, 'Motor axis 2': 346}\n",
      "Motor found in d:/flagellar/data//train/tomo_89d156 at position: z=139, y=407, x=346\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_08bf73 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.432s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.127s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.100s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.113s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_08bf73', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_08bf73 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a2a928 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.499s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.127s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.144s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.108s\n",
      "[PROFILE] Inference batch 3/4: 0.114s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.178s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.045s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a2a928', 'Motor axis 0': 128, 'Motor axis 1': 507, 'Motor axis 2': 651}\n",
      "Motor found in d:/flagellar/data//train/tomo_a2a928 at position: z=128, y=507, x=651\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_50f0bf (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.506s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 2.347s\n",
      "[PROFILE] Inference batch 2/4: 0.069s\n",
      "[PROFILE] Inference batch 3/4: 0.085s\n",
      "[PROFILE] Inference batch 4/4: 0.082s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_50f0bf', 'Motor axis 0': 205, 'Motor axis 1': 265, 'Motor axis 2': 708}\n",
      "Motor found in d:/flagellar/data//train/tomo_50f0bf at position: z=205, y=265, x=708\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_b7d94c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.552s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.224s\n",
      "[PROFILE] Inference batch 3/4: 0.107s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.127s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.126s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 2.183s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_b7d94c', 'Motor axis 0': 105, 'Motor axis 1': 747, 'Motor axis 2': 655}\n",
      "Motor found in d:/flagellar/data//train/tomo_b7d94c at position: z=105, y=747, x=655\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_be4a3a (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.556s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.104s\n",
      "[PROFILE] Inference batch 3/4: 0.138s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 2.142s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_be4a3a', 'Motor axis 0': 113, 'Motor axis 1': 605, 'Motor axis 2': 899}\n",
      "Motor found in d:/flagellar/data//train/tomo_be4a3a at position: z=113, y=605, x=899\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_10c564 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.517s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.130s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 2.154s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_10c564', 'Motor axis 0': 19, 'Motor axis 1': 455, 'Motor axis 2': 265}\n",
      "Motor found in d:/flagellar/data//train/tomo_10c564 at position: z=19, y=455, x=265\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_ef1a1a (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.474s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 2.193s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_ef1a1a', 'Motor axis 0': 132, 'Motor axis 1': 351, 'Motor axis 2': 405}\n",
      "Motor found in d:/flagellar/data//train/tomo_ef1a1a at position: z=132, y=351, x=405\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cc3fc4 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.463s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.215s\n",
      "[PROFILE] Inference batch 3/4: 0.130s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.117s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 2.148s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cc3fc4', 'Motor axis 0': 199, 'Motor axis 1': 387, 'Motor axis 2': 215}\n",
      "Motor found in d:/flagellar/data//train/tomo_cc3fc4 at position: z=199, y=387, x=215\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_8e4f7d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.532s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.121s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 2.158s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_8e4f7d', 'Motor axis 0': 129, 'Motor axis 1': 591, 'Motor axis 2': 478}\n",
      "Motor found in d:/flagellar/data//train/tomo_8e4f7d at position: z=129, y=591, x=478\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_146de2 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.449s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.215s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 2.285s\n",
      "[PROFILE] Inference batch 2/4: 0.069s\n",
      "[PROFILE] Inference batch 3/4: 0.088s\n",
      "[PROFILE] Inference batch 4/4: 0.070s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_146de2', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_146de2 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_8c13d9 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.458s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.146s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 2.177s\n",
      "[PROFILE] Inference batch 2/4: 0.053s\n",
      "[PROFILE] Inference batch 3/4: 0.048s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_8c13d9', 'Motor axis 0': 121, 'Motor axis 1': 447, 'Motor axis 2': 291}\n",
      "Motor found in d:/flagellar/data//train/tomo_8c13d9 at position: z=121, y=447, x=291\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_71ece1 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.491s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.111s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 2.180s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.054s\n",
      "[PROFILE] Inference batch 4/4: 0.046s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_71ece1', 'Motor axis 0': 146, 'Motor axis 1': 421, 'Motor axis 2': 416}\n",
      "Motor found in d:/flagellar/data//train/tomo_71ece1 at position: z=146, y=421, x=416\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_821255 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.493s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.129s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.121s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.127s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 2.133s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_821255', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_821255 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_91031e (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.446s\n",
      "[PROFILE] Inference batch 2/4: 0.142s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.218s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 2.150s\n",
      "[PROFILE] Inference batch 2/4: 0.045s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_91031e', 'Motor axis 0': 81, 'Motor axis 1': 437, 'Motor axis 2': 598}\n",
      "Motor found in d:/flagellar/data//train/tomo_91031e at position: z=81, y=437, x=598\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_4f5a7b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.444s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.211s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.109s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 2.147s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_4f5a7b', 'Motor axis 0': 165, 'Motor axis 1': 517, 'Motor axis 2': 803}\n",
      "Motor found in d:/flagellar/data//train/tomo_4f5a7b at position: z=165, y=517, x=803\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_fd5b38 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.625s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 2.199s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.052s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_fd5b38', 'Motor axis 0': 181, 'Motor axis 1': 520, 'Motor axis 2': 628}\n",
      "Motor found in d:/flagellar/data//train/tomo_fd5b38 at position: z=181, y=520, x=628\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_081a2d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.491s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.134s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.148s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 2.175s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_081a2d', 'Motor axis 0': 164, 'Motor axis 1': 314, 'Motor axis 2': 654}\n",
      "Motor found in d:/flagellar/data//train/tomo_081a2d at position: z=164, y=314, x=654\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dfc627 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.534s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.110s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.108s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.125s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.148s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.135s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 2.197s\n",
      "[PROFILE] Inference batch 2/4: 0.055s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dfc627', 'Motor axis 0': 101, 'Motor axis 1': 354, 'Motor axis 2': 767}\n",
      "Motor found in d:/flagellar/data//train/tomo_dfc627 at position: z=101, y=354, x=767\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_5b087f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.527s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.231s\n",
      "[PROFILE] Inference batch 2/4: 0.218s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.124s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.122s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 2.162s\n",
      "[PROFILE] Inference batch 2/4: 0.048s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_5b087f', 'Motor axis 0': 162, 'Motor axis 1': 252, 'Motor axis 2': 373}\n",
      "Motor found in d:/flagellar/data//train/tomo_5b087f at position: z=162, y=252, x=373\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c38e83 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.550s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.210s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.211s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.141s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.111s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.121s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 2.284s\n",
      "[PROFILE] Inference batch 2/4: 0.068s\n",
      "[PROFILE] Inference batch 3/4: 0.075s\n",
      "[PROFILE] Inference batch 4/4: 0.072s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c38e83', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_c38e83 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_8d231b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.481s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 2.130s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_8d231b', 'Motor axis 0': 121, 'Motor axis 1': 359, 'Motor axis 2': 838}\n",
      "Motor found in d:/flagellar/data//train/tomo_8d231b at position: z=121, y=359, x=838\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_1c75ac (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.485s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 2.184s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.039s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_1c75ac', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_1c75ac at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_5764d6 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.510s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.214s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.221s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.217s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.223s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 2.319s\n",
      "[PROFILE] Inference batch 2/4: 0.068s\n",
      "[PROFILE] Inference batch 3/4: 0.083s\n",
      "[PROFILE] Inference batch 4/4: 0.067s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_5764d6', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_5764d6 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_51a77e (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.461s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.228s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.145s\n",
      "[PROFILE] Inference batch 2/4: 0.215s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 2.272s\n",
      "[PROFILE] Inference batch 2/4: 0.074s\n",
      "[PROFILE] Inference batch 3/4: 0.069s\n",
      "[PROFILE] Inference batch 4/4: 0.095s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_51a77e', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_51a77e at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_417e5f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.492s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 2.153s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_417e5f', 'Motor axis 0': 238, 'Motor axis 1': 415, 'Motor axis 2': 619}\n",
      "Motor found in d:/flagellar/data//train/tomo_417e5f at position: z=238, y=415, x=619\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_229f0a (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.455s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 2.308s\n",
      "[PROFILE] Inference batch 2/4: 0.071s\n",
      "[PROFILE] Inference batch 3/4: 0.098s\n",
      "[PROFILE] Inference batch 4/4: 0.077s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_229f0a', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_229f0a at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_675583 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.509s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.130s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.109s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 2.170s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.054s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_675583', 'Motor axis 0': 188, 'Motor axis 1': 565, 'Motor axis 2': 764}\n",
      "Motor found in d:/flagellar/data//train/tomo_675583 at position: z=188, y=565, x=764\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_e764a7 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.456s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.192s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 2.310s\n",
      "[PROFILE] Inference batch 2/4: 0.071s\n",
      "[PROFILE] Inference batch 3/4: 0.099s\n",
      "[PROFILE] Inference batch 4/4: 0.100s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_e764a7', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_e764a7 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_3a0914 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.519s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.189s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.195s\n",
      "[PROFILE] Inference batch 1/4: 0.212s\n",
      "[PROFILE] Inference batch 2/4: 0.270s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.190s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.210s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.190s\n",
      "[PROFILE] Inference batch 1/4: 0.212s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.189s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.222s\n",
      "[PROFILE] Inference batch 2/4: 0.225s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.190s\n",
      "[PROFILE] Inference batch 1/4: 0.219s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.189s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.193s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.198s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.212s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.184s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_3a0914', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_3a0914 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f6de9b (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.534s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.211s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.210s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.216s\n",
      "[PROFILE] Inference batch 2/4: 0.224s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.148s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f6de9b', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_f6de9b at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c4a4bb (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.599s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 2.221s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c4a4bb', 'Motor axis 0': 128, 'Motor axis 1': 551, 'Motor axis 2': 157}\n",
      "Motor found in d:/flagellar/data//train/tomo_c4a4bb at position: z=128, y=551, x=157\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_5984bf (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.769s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.130s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.184s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_5984bf', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_5984bf at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_bde7f3 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.533s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.138s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.122s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 2.146s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_bde7f3', 'Motor axis 0': 263, 'Motor axis 1': 531, 'Motor axis 2': 289}\n",
      "Motor found in d:/flagellar/data//train/tomo_bde7f3 at position: z=263, y=531, x=289\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f7f28b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.437s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 2.189s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f7f28b', 'Motor axis 0': 162, 'Motor axis 1': 425, 'Motor axis 2': 281}\n",
      "Motor found in d:/flagellar/data//train/tomo_f7f28b at position: z=162, y=425, x=281\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_221c8e (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.495s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 2.240s\n",
      "[PROFILE] Inference batch 2/4: 0.068s\n",
      "[PROFILE] Inference batch 3/4: 0.076s\n",
      "[PROFILE] Inference batch 4/4: 0.093s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_221c8e', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_221c8e at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_412d88 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.500s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.189s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.217s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.209s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 2.166s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_412d88', 'Motor axis 0': 54, 'Motor axis 1': 679, 'Motor axis 2': 12}\n",
      "Motor found in d:/flagellar/data//train/tomo_412d88 at position: z=54, y=679, x=12\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_0308c5 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.512s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.122s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 2.240s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.054s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_0308c5', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_0308c5 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c8f3ce (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.520s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.125s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.546s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 2.405s\n",
      "[PROFILE] Inference batch 2/4: 0.057s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c8f3ce', 'Motor axis 0': 184, 'Motor axis 1': 718, 'Motor axis 2': 674}\n",
      "Motor found in d:/flagellar/data//train/tomo_c8f3ce at position: z=184, y=718, x=674\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a4c52f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.597s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.120s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 2.216s\n",
      "[PROFILE] Inference batch 2/4: 0.058s\n",
      "[PROFILE] Inference batch 3/4: 0.058s\n",
      "[PROFILE] Inference batch 4/4: 0.058s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a4c52f', 'Motor axis 0': 100, 'Motor axis 1': 420, 'Motor axis 2': 400}\n",
      "Motor found in d:/flagellar/data//train/tomo_a4c52f at position: z=100, y=420, x=400\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a3ed10 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.670s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.211s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.189s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.209s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.191s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.143s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.219s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.189s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a3ed10', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_a3ed10 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_423d52 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.682s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.133s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 2.169s\n",
      "[PROFILE] Inference batch 2/4: 0.051s\n",
      "[PROFILE] Inference batch 3/4: 0.038s\n",
      "[PROFILE] Inference batch 4/4: 0.052s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_423d52', 'Motor axis 0': 151, 'Motor axis 1': 443, 'Motor axis 2': 455}\n",
      "Motor found in d:/flagellar/data//train/tomo_423d52 at position: z=151, y=443, x=455\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_256717 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.568s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 2.191s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_256717', 'Motor axis 0': 176, 'Motor axis 1': 769, 'Motor axis 2': 514}\n",
      "Motor found in d:/flagellar/data//train/tomo_256717 at position: z=176, y=769, x=514\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_9dbc12 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.549s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.208s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.136s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.226s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.219s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 2.319s\n",
      "[PROFILE] Inference batch 2/4: 0.069s\n",
      "[PROFILE] Inference batch 3/4: 0.069s\n",
      "[PROFILE] Inference batch 4/4: 0.094s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_9dbc12', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_9dbc12 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_e3864f (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.618s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.144s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.119s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_e3864f', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_e3864f at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_1fb6a7 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.620s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.122s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.125s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 2.177s\n",
      "[PROFILE] Inference batch 2/4: 0.050s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_1fb6a7', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_1fb6a7 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_e26c6b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.570s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.135s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 2.170s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.050s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_e26c6b', 'Motor axis 0': 142, 'Motor axis 1': 177, 'Motor axis 2': 319}\n",
      "Motor found in d:/flagellar/data//train/tomo_e26c6b at position: z=142, y=177, x=319\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_648adf (1/1)\n",
      "Processing 400 out of 400 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.556s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.127s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 2.274s\n",
      "[PROFILE] Inference batch 2/4: 0.060s\n",
      "[PROFILE] Inference batch 3/4: 0.077s\n",
      "[PROFILE] Inference batch 4/4: 0.074s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_648adf', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_648adf at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_fe85f6 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.601s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.204s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.215s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.183s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.219s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.129s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.136s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.183s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_fe85f6', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_fe85f6 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_8f5995 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.634s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.120s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 2.177s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_8f5995', 'Motor axis 0': 161, 'Motor axis 1': 328, 'Motor axis 2': 218}\n",
      "Motor found in d:/flagellar/data//train/tomo_8f5995 at position: z=161, y=328, x=218\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_385eb6 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.572s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 2.206s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_385eb6', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_385eb6 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2c607f (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.602s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.150s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.116s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.105s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 2.164s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.051s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2c607f', 'Motor axis 0': 112, 'Motor axis 1': 346, 'Motor axis 2': 526}\n",
      "Motor found in d:/flagellar/data//train/tomo_2c607f at position: z=112, y=346, x=526\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_d0aa3b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.701s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 2.220s\n",
      "[PROFILE] Inference batch 2/4: 0.045s\n",
      "[PROFILE] Inference batch 3/4: 0.053s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_d0aa3b', 'Motor axis 0': 163, 'Motor axis 1': 627, 'Motor axis 2': 794}\n",
      "Motor found in d:/flagellar/data//train/tomo_d0aa3b at position: z=163, y=627, x=794\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_285d15 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.687s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.183s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.244s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.109s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 2.215s\n",
      "[PROFILE] Inference batch 2/4: 0.056s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_285d15', 'Motor axis 0': 137, 'Motor axis 1': 386, 'Motor axis 2': 613}\n",
      "Motor found in d:/flagellar/data//train/tomo_285d15 at position: z=137, y=386, x=613\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2fc82d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.528s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.121s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.108s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.210s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 2.278s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2fc82d', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_2fc82d at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2e1f4c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.609s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.112s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 2.201s\n",
      "[PROFILE] Inference batch 2/4: 0.053s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2e1f4c', 'Motor axis 0': 70, 'Motor axis 1': 87, 'Motor axis 2': 332}\n",
      "Motor found in d:/flagellar/data//train/tomo_2e1f4c at position: z=70, y=87, x=332\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f672c0 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.520s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.185s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.226s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.212s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.197s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.239s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.185s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.229s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.218s\n",
      "[PROFILE] Inference batch 3/4: 0.141s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.212s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.144s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.218s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.209s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.194s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.196s\n",
      "[PROFILE] Inference batch 4/4: 0.199s\n",
      "[PROFILE] Inference batch 1/4: 0.226s\n",
      "[PROFILE] Inference batch 2/4: 0.226s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.192s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.221s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.193s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f672c0', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_f672c0 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a8bf76 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.666s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.109s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 2.249s\n",
      "[PROFILE] Inference batch 2/4: 0.043s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a8bf76', 'Motor axis 0': 140, 'Motor axis 1': 334, 'Motor axis 2': 383}\n",
      "Motor found in d:/flagellar/data//train/tomo_a8bf76 at position: z=140, y=334, x=383\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_374ca7 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 5.248s\n",
      "[PROFILE] Inference batch 2/4: 0.211s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.191s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.196s\n",
      "[PROFILE] Inference batch 4/4: 0.201s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.188s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.191s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.201s\n",
      "[PROFILE] Inference batch 4/4: 0.191s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.218s\n",
      "[PROFILE] Inference batch 3/4: 0.202s\n",
      "[PROFILE] Inference batch 4/4: 0.199s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.223s\n",
      "[PROFILE] Inference batch 3/4: 0.196s\n",
      "[PROFILE] Inference batch 4/4: 0.199s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.225s\n",
      "[PROFILE] Inference batch 3/4: 0.194s\n",
      "[PROFILE] Inference batch 4/4: 0.195s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.137s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.217s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.242s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.124s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_374ca7', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_374ca7 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_172f08 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.684s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.217s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.190s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.219s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.185s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.228s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.191s\n",
      "[PROFILE] Inference batch 1/4: 0.208s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.209s\n",
      "[PROFILE] Inference batch 2/4: 0.221s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.136s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.126s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.154s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.141s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_172f08', 'Motor axis 0': 373, 'Motor axis 1': 537, 'Motor axis 2': 899}\n",
      "Motor found in d:/flagellar/data//train/tomo_172f08 at position: z=373, y=537, x=899\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_3b8291 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.609s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.194s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.139s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.124s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.125s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 2.326s\n",
      "[PROFILE] Inference batch 2/4: 0.080s\n",
      "[PROFILE] Inference batch 3/4: 0.092s\n",
      "[PROFILE] Inference batch 4/4: 0.068s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_3b8291', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_3b8291 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_891afe (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.716s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.120s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 2.215s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.057s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_891afe', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_891afe at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2645a0 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.503s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 2.223s\n",
      "[PROFILE] Inference batch 2/4: 0.055s\n",
      "[PROFILE] Inference batch 3/4: 0.042s\n",
      "[PROFILE] Inference batch 4/4: 0.057s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2645a0', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_2645a0 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_04d42b (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.535s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.194s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.212s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.189s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.242s\n",
      "[PROFILE] Inference batch 3/4: 0.223s\n",
      "[PROFILE] Inference batch 4/4: 0.213s\n",
      "[PROFILE] Inference batch 1/4: 0.235s\n",
      "[PROFILE] Inference batch 2/4: 0.228s\n",
      "[PROFILE] Inference batch 3/4: 0.231s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.244s\n",
      "[PROFILE] Inference batch 2/4: 0.238s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.217s\n",
      "[PROFILE] Inference batch 1/4: 0.243s\n",
      "[PROFILE] Inference batch 2/4: 0.256s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.216s\n",
      "[PROFILE] Inference batch 1/4: 0.236s\n",
      "[PROFILE] Inference batch 2/4: 0.259s\n",
      "[PROFILE] Inference batch 3/4: 0.238s\n",
      "[PROFILE] Inference batch 4/4: 0.192s\n",
      "[PROFILE] Inference batch 1/4: 0.234s\n",
      "[PROFILE] Inference batch 2/4: 0.233s\n",
      "[PROFILE] Inference batch 3/4: 0.240s\n",
      "[PROFILE] Inference batch 4/4: 0.212s\n",
      "[PROFILE] Inference batch 1/4: 0.274s\n",
      "[PROFILE] Inference batch 2/4: 0.285s\n",
      "[PROFILE] Inference batch 3/4: 0.237s\n",
      "[PROFILE] Inference batch 4/4: 0.229s\n",
      "[PROFILE] Inference batch 1/4: 0.270s\n",
      "[PROFILE] Inference batch 2/4: 0.279s\n",
      "[PROFILE] Inference batch 3/4: 0.239s\n",
      "[PROFILE] Inference batch 4/4: 0.234s\n",
      "[PROFILE] Inference batch 1/4: 0.259s\n",
      "[PROFILE] Inference batch 2/4: 0.275s\n",
      "[PROFILE] Inference batch 3/4: 0.234s\n",
      "[PROFILE] Inference batch 4/4: 0.223s\n",
      "[PROFILE] Inference batch 1/4: 0.238s\n",
      "[PROFILE] Inference batch 2/4: 0.294s\n",
      "[PROFILE] Inference batch 3/4: 0.240s\n",
      "[PROFILE] Inference batch 4/4: 0.203s\n",
      "[PROFILE] Inference batch 1/4: 0.274s\n",
      "[PROFILE] Inference batch 2/4: 0.272s\n",
      "[PROFILE] Inference batch 3/4: 0.250s\n",
      "[PROFILE] Inference batch 4/4: 0.237s\n",
      "[PROFILE] Inference batch 1/4: 0.253s\n",
      "[PROFILE] Inference batch 2/4: 0.290s\n",
      "[PROFILE] Inference batch 3/4: 0.205s\n",
      "[PROFILE] Inference batch 4/4: 0.232s\n",
      "[PROFILE] Inference batch 1/4: 0.241s\n",
      "[PROFILE] Inference batch 2/4: 0.259s\n",
      "[PROFILE] Inference batch 3/4: 0.220s\n",
      "[PROFILE] Inference batch 4/4: 0.234s\n",
      "[PROFILE] Inference batch 1/4: 2.393s\n",
      "[PROFILE] Inference batch 2/4: 0.088s\n",
      "[PROFILE] Inference batch 3/4: 0.123s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_04d42b', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_04d42b at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_a2bf30 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.681s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.119s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 2.188s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_a2bf30', 'Motor axis 0': 151, 'Motor axis 1': 260, 'Motor axis 2': 465}\n",
      "Motor found in d:/flagellar/data//train/tomo_a2bf30 at position: z=151, y=260, x=465\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_cc2b5c (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.626s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.149s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.102s\n",
      "[PROFILE] Inference batch 3/4: 0.104s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 2.165s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.054s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_cc2b5c', 'Motor axis 0': 124, 'Motor axis 1': 443, 'Motor axis 2': 473}\n",
      "Motor found in d:/flagellar/data//train/tomo_cc2b5c at position: z=124, y=443, x=473\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f94504 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.614s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.139s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 2.376s\n",
      "[PROFILE] Inference batch 2/4: 0.086s\n",
      "[PROFILE] Inference batch 3/4: 0.083s\n",
      "[PROFILE] Inference batch 4/4: 0.090s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f94504', 'Motor axis 0': 272, 'Motor axis 1': 196, 'Motor axis 2': 399}\n",
      "Motor found in d:/flagellar/data//train/tomo_f94504 at position: z=272, y=196, x=399\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_711fad (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.562s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.137s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.189s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.199s\n",
      "[PROFILE] Inference batch 4/4: 0.192s\n",
      "[PROFILE] Inference batch 1/4: 0.227s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 2.378s\n",
      "[PROFILE] Inference batch 2/4: 0.071s\n",
      "[PROFILE] Inference batch 3/4: 0.094s\n",
      "[PROFILE] Inference batch 4/4: 0.090s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_711fad', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_711fad at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_221a47 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.633s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.173s\n",
      "[PROFILE] Inference batch 2/4: 0.054s\n",
      "[PROFILE] Inference batch 3/4: 0.053s\n",
      "[PROFILE] Inference batch 4/4: 0.055s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_221a47', 'Motor axis 0': 162, 'Motor axis 1': 323, 'Motor axis 2': 564}\n",
      "Motor found in d:/flagellar/data//train/tomo_221a47 at position: z=162, y=323, x=564\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2483bb (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.571s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.185s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 2.189s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.058s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2483bb', 'Motor axis 0': 144, 'Motor axis 1': 618, 'Motor axis 2': 482}\n",
      "Motor found in d:/flagellar/data//train/tomo_2483bb at position: z=144, y=618, x=482\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_f8b835 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.585s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.201s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.212s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.217s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.214s\n",
      "[PROFILE] Inference batch 3/4: 0.186s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_f8b835', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_f8b835 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c36b4b (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.701s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.194s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.184s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.194s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.147s\n",
      "[PROFILE] Inference batch 2/4: 0.144s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 2.486s\n",
      "[PROFILE] Inference batch 2/4: 0.039s\n",
      "[PROFILE] Inference batch 3/4: 0.039s\n",
      "[PROFILE] Inference batch 4/4: 0.047s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c36b4b', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_c36b4b at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_d7475d (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.882s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.124s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 2.158s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_d7475d', 'Motor axis 0': 146, 'Motor axis 1': 218, 'Motor axis 2': 439}\n",
      "Motor found in d:/flagellar/data//train/tomo_d7475d at position: z=146, y=218, x=439\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_57c814 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.549s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.197s\n",
      "[PROFILE] Inference batch 2/4: 0.216s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.213s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.136s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 2.142s\n",
      "[PROFILE] Inference batch 2/4: 0.038s\n",
      "[PROFILE] Inference batch 3/4: 0.038s\n",
      "[PROFILE] Inference batch 4/4: 0.038s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_57c814', 'Motor axis 0': 166, 'Motor axis 1': 357, 'Motor axis 2': 402}\n",
      "Motor found in d:/flagellar/data//train/tomo_57c814 at position: z=166, y=357, x=402\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_50cbd9 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.542s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.123s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.215s\n",
      "[PROFILE] Inference batch 2/4: 0.215s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.224s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.199s\n",
      "[PROFILE] Inference batch 3/4: 0.197s\n",
      "[PROFILE] Inference batch 4/4: 0.186s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.182s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.163s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.197s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.155s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.172s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.152s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.138s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_50cbd9', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_50cbd9 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_05f919 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.587s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.138s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 2.165s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_05f919', 'Motor axis 0': 125, 'Motor axis 1': 475, 'Motor axis 2': 839}\n",
      "Motor found in d:/flagellar/data//train/tomo_05f919 at position: z=125, y=475, x=839\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_d8c917 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.590s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.145s\n",
      "[PROFILE] Inference batch 4/4: 0.183s\n",
      "[PROFILE] Inference batch 1/4: 0.184s\n",
      "[PROFILE] Inference batch 2/4: 0.208s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.153s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.191s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.111s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.203s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.184s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.205s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.181s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.212s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.145s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.125s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_d8c917', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_d8c917 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_80bf0f (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.680s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.171s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.130s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.140s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.115s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.133s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 2.333s\n",
      "[PROFILE] Inference batch 2/4: 0.071s\n",
      "[PROFILE] Inference batch 3/4: 0.095s\n",
      "[PROFILE] Inference batch 4/4: 0.069s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_80bf0f', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_80bf0f at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_e7c195 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.596s\n",
      "[PROFILE] Inference batch 2/4: 0.140s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.127s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.218s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.164s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.102s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.178s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.146s\n",
      "[PROFILE] Inference batch 2/4: 0.143s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 2.195s\n",
      "[PROFILE] Inference batch 2/4: 0.053s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.054s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_e7c195', 'Motor axis 0': 168, 'Motor axis 1': 654, 'Motor axis 2': 590}\n",
      "Motor found in d:/flagellar/data//train/tomo_e7c195 at position: z=168, y=654, x=590\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_fd41c4 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.544s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.105s\n",
      "[PROFILE] Inference batch 4/4: 0.137s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.109s\n",
      "[PROFILE] Inference batch 4/4: 0.104s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.118s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.105s\n",
      "[PROFILE] Inference batch 1/4: 2.161s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_fd41c4', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_fd41c4 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_bdd3a0 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.590s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.204s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.140s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.174s\n",
      "[PROFILE] Inference batch 4/4: 0.116s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.179s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 2.204s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.055s\n",
      "[PROFILE] Inference batch 4/4: 0.056s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_bdd3a0', 'Motor axis 0': 143, 'Motor axis 1': 812, 'Motor axis 2': 589}\n",
      "Motor found in d:/flagellar/data//train/tomo_bdd3a0 at position: z=143, y=812, x=589\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_7fa3b1 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.589s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.142s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.148s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.199s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.190s\n",
      "[PROFILE] Inference batch 2/4: 0.192s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.196s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.123s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.135s\n",
      "[PROFILE] Inference batch 1/4: 2.169s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.043s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_7fa3b1', 'Motor axis 0': 72, 'Motor axis 1': 445, 'Motor axis 2': 423}\n",
      "Motor found in d:/flagellar/data//train/tomo_7fa3b1 at position: z=72, y=445, x=423\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_305c97 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.498s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.207s\n",
      "[PROFILE] Inference batch 2/4: 0.180s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.208s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.128s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.182s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.126s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.131s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 2.149s\n",
      "[PROFILE] Inference batch 2/4: 0.040s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.041s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_305c97', 'Motor axis 0': 64, 'Motor axis 1': 683, 'Motor axis 2': 409}\n",
      "Motor found in d:/flagellar/data//train/tomo_305c97 at position: z=64, y=683, x=409\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_2acf68 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.568s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.144s\n",
      "[PROFILE] Inference batch 4/4: 0.101s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.185s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.152s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.113s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.106s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.128s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.132s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 2.170s\n",
      "[PROFILE] Inference batch 2/4: 0.042s\n",
      "[PROFILE] Inference batch 3/4: 0.056s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_2acf68', 'Motor axis 0': 136, 'Motor axis 1': 338, 'Motor axis 2': 678}\n",
      "Motor found in d:/flagellar/data//train/tomo_2acf68 at position: z=136, y=338, x=678\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_b8f096 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.477s\n",
      "[PROFILE] Inference batch 2/4: 0.145s\n",
      "[PROFILE] Inference batch 3/4: 0.151s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.138s\n",
      "[PROFILE] Inference batch 2/4: 0.147s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.175s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.117s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.150s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.110s\n",
      "[PROFILE] Inference batch 1/4: 2.169s\n",
      "[PROFILE] Inference batch 2/4: 0.046s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_b8f096', 'Motor axis 0': 142, 'Motor axis 1': 601, 'Motor axis 2': 586}\n",
      "Motor found in d:/flagellar/data//train/tomo_b8f096 at position: z=142, y=601, x=586\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_891730 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.498s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.157s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.205s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.208s\n",
      "[PROFILE] Inference batch 2/4: 0.209s\n",
      "[PROFILE] Inference batch 3/4: 0.132s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.159s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.159s\n",
      "[PROFILE] Inference batch 1/4: 2.270s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.041s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_891730', 'Motor axis 0': 239, 'Motor axis 1': 193, 'Motor axis 2': 383}\n",
      "Motor found in d:/flagellar/data//train/tomo_891730 at position: z=239, y=193, x=383\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_97a2c6 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.522s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.182s\n",
      "[PROFILE] Inference batch 4/4: 0.187s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.142s\n",
      "[PROFILE] Inference batch 4/4: 0.125s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.126s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.116s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.189s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.162s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.169s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.129s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.114s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 2.182s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.040s\n",
      "[PROFILE] Inference batch 4/4: 0.045s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_97a2c6', 'Motor axis 0': 152, 'Motor axis 1': 684, 'Motor axis 2': 637}\n",
      "Motor found in d:/flagellar/data//train/tomo_97a2c6 at position: z=152, y=684, x=637\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_c925ee (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.583s\n",
      "[PROFILE] Inference batch 2/4: 0.150s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.134s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.151s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.132s\n",
      "[PROFILE] Inference batch 1/4: 0.159s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.156s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.109s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.156s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.167s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.155s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.144s\n",
      "[PROFILE] Inference batch 1/4: 0.166s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.166s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.163s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.185s\n",
      "[PROFILE] Inference batch 2/4: 0.128s\n",
      "[PROFILE] Inference batch 3/4: 0.154s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.124s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.194s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.164s\n",
      "[PROFILE] Inference batch 1/4: 0.198s\n",
      "[PROFILE] Inference batch 2/4: 0.198s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.202s\n",
      "[PROFILE] Inference batch 2/4: 0.186s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.149s\n",
      "[PROFILE] Inference batch 3/4: 0.147s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.153s\n",
      "[PROFILE] Inference batch 2/4: 0.122s\n",
      "[PROFILE] Inference batch 3/4: 0.150s\n",
      "[PROFILE] Inference batch 4/4: 0.134s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_c925ee', 'Motor axis 0': 336, 'Motor axis 1': 471, 'Motor axis 2': 40}\n",
      "Motor found in d:/flagellar/data//train/tomo_c925ee at position: z=336, y=471, x=40\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_0eb994 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.669s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.121s\n",
      "[PROFILE] Inference batch 1/4: 0.160s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.181s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.187s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.201s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.184s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "[PROFILE] Inference batch 1/4: 2.187s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.048s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_0eb994', 'Motor axis 0': 159, 'Motor axis 1': 834, 'Motor axis 2': 632}\n",
      "Motor found in d:/flagellar/data//train/tomo_0eb994 at position: z=159, y=834, x=632\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_ab30af (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.653s\n",
      "[PROFILE] Inference batch 2/4: 0.196s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.225s\n",
      "[PROFILE] Inference batch 2/4: 0.230s\n",
      "[PROFILE] Inference batch 3/4: 0.188s\n",
      "[PROFILE] Inference batch 4/4: 0.192s\n",
      "[PROFILE] Inference batch 1/4: 0.214s\n",
      "[PROFILE] Inference batch 2/4: 0.255s\n",
      "[PROFILE] Inference batch 3/4: 0.223s\n",
      "[PROFILE] Inference batch 4/4: 0.215s\n",
      "[PROFILE] Inference batch 1/4: 0.233s\n",
      "[PROFILE] Inference batch 2/4: 0.251s\n",
      "[PROFILE] Inference batch 3/4: 0.229s\n",
      "[PROFILE] Inference batch 4/4: 0.204s\n",
      "[PROFILE] Inference batch 1/4: 0.239s\n",
      "[PROFILE] Inference batch 2/4: 0.253s\n",
      "[PROFILE] Inference batch 3/4: 0.239s\n",
      "[PROFILE] Inference batch 4/4: 0.206s\n",
      "[PROFILE] Inference batch 1/4: 0.230s\n",
      "[PROFILE] Inference batch 2/4: 0.236s\n",
      "[PROFILE] Inference batch 3/4: 0.235s\n",
      "[PROFILE] Inference batch 4/4: 0.190s\n",
      "[PROFILE] Inference batch 1/4: 0.257s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.240s\n",
      "[PROFILE] Inference batch 4/4: 0.228s\n",
      "[PROFILE] Inference batch 1/4: 0.248s\n",
      "[PROFILE] Inference batch 2/4: 0.267s\n",
      "[PROFILE] Inference batch 3/4: 0.241s\n",
      "[PROFILE] Inference batch 4/4: 0.208s\n",
      "[PROFILE] Inference batch 1/4: 0.228s\n",
      "[PROFILE] Inference batch 2/4: 0.221s\n",
      "[PROFILE] Inference batch 3/4: 0.237s\n",
      "[PROFILE] Inference batch 4/4: 0.229s\n",
      "[PROFILE] Inference batch 1/4: 0.256s\n",
      "[PROFILE] Inference batch 2/4: 0.221s\n",
      "[PROFILE] Inference batch 3/4: 0.246s\n",
      "[PROFILE] Inference batch 4/4: 0.219s\n",
      "[PROFILE] Inference batch 1/4: 0.252s\n",
      "[PROFILE] Inference batch 2/4: 0.297s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.235s\n",
      "[PROFILE] Inference batch 1/4: 0.240s\n",
      "[PROFILE] Inference batch 2/4: 0.261s\n",
      "[PROFILE] Inference batch 3/4: 0.231s\n",
      "[PROFILE] Inference batch 4/4: 0.226s\n",
      "[PROFILE] Inference batch 1/4: 0.252s\n",
      "[PROFILE] Inference batch 2/4: 0.274s\n",
      "[PROFILE] Inference batch 3/4: 0.210s\n",
      "[PROFILE] Inference batch 4/4: 0.232s\n",
      "[PROFILE] Inference batch 1/4: 0.260s\n",
      "[PROFILE] Inference batch 2/4: 0.261s\n",
      "[PROFILE] Inference batch 3/4: 0.221s\n",
      "[PROFILE] Inference batch 4/4: 0.217s\n",
      "[PROFILE] Inference batch 1/4: 0.255s\n",
      "[PROFILE] Inference batch 2/4: 0.202s\n",
      "[PROFILE] Inference batch 3/4: 0.229s\n",
      "[PROFILE] Inference batch 4/4: 0.240s\n",
      "[PROFILE] Inference batch 1/4: 2.315s\n",
      "[PROFILE] Inference batch 2/4: 0.107s\n",
      "[PROFILE] Inference batch 3/4: 0.099s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_ab30af', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_ab30af at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_676744 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.560s\n",
      "[PROFILE] Inference batch 2/4: 0.159s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.141s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.158s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.207s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.169s\n",
      "[PROFILE] Inference batch 1/4: 0.169s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.181s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.196s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.206s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.190s\n",
      "[PROFILE] Inference batch 4/4: 0.179s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.172s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 2.201s\n",
      "[PROFILE] Inference batch 2/4: 0.055s\n",
      "[PROFILE] Inference batch 3/4: 0.058s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_676744', 'Motor axis 0': 246, 'Motor axis 1': 593, 'Motor axis 2': 491}\n",
      "Motor found in d:/flagellar/data//train/tomo_676744 at position: z=246, y=593, x=491\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_b98cf6 (1/1)\n",
      "Processing 800 out of 800 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.619s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.197s\n",
      "[PROFILE] Inference batch 4/4: 0.155s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.157s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.208s\n",
      "[PROFILE] Inference batch 2/4: 0.206s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.184s\n",
      "[PROFILE] Inference batch 1/4: 0.194s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.192s\n",
      "[PROFILE] Inference batch 4/4: 0.187s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.181s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.193s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.176s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.162s\n",
      "[PROFILE] Inference batch 2/4: 0.110s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.148s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.178s\n",
      "[PROFILE] Inference batch 1/4: 0.188s\n",
      "[PROFILE] Inference batch 2/4: 0.187s\n",
      "[PROFILE] Inference batch 3/4: 0.143s\n",
      "[PROFILE] Inference batch 4/4: 0.170s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.168s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.162s\n",
      "[PROFILE] Inference batch 3/4: 0.149s\n",
      "[PROFILE] Inference batch 4/4: 0.149s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.161s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.147s\n",
      "[PROFILE] Inference batch 1/4: 0.189s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.171s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.245s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.176s\n",
      "[PROFILE] Inference batch 2/4: 0.210s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.164s\n",
      "[PROFILE] Inference batch 2/4: 0.173s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.133s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.175s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.197s\n",
      "[PROFILE] Inference batch 3/4: 0.146s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.165s\n",
      "[PROFILE] Inference batch 4/4: 0.174s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_b98cf6', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_b98cf6 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_6303f0 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.528s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.152s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.139s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.114s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.130s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.183s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.180s\n",
      "[PROFILE] Inference batch 1/4: 0.210s\n",
      "[PROFILE] Inference batch 2/4: 0.193s\n",
      "[PROFILE] Inference batch 3/4: 0.178s\n",
      "[PROFILE] Inference batch 4/4: 0.177s\n",
      "[PROFILE] Inference batch 1/4: 0.220s\n",
      "[PROFILE] Inference batch 2/4: 0.200s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.191s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.160s\n",
      "[PROFILE] Inference batch 1/4: 0.186s\n",
      "[PROFILE] Inference batch 2/4: 0.195s\n",
      "[PROFILE] Inference batch 3/4: 0.167s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.192s\n",
      "[PROFILE] Inference batch 2/4: 0.160s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.146s\n",
      "[PROFILE] Inference batch 1/4: 2.210s\n",
      "[PROFILE] Inference batch 2/4: 0.041s\n",
      "[PROFILE] Inference batch 3/4: 0.048s\n",
      "[PROFILE] Inference batch 4/4: 0.040s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_6303f0', 'Motor axis 0': 200, 'Motor axis 1': 590, 'Motor axis 2': 211}\n",
      "Motor found in d:/flagellar/data//train/tomo_6303f0 at position: z=200, y=590, x=211\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_eb4fd4 (1/1)\n",
      "Processing 300 out of 300 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.677s\n",
      "[PROFILE] Inference batch 2/4: 0.148s\n",
      "[PROFILE] Inference batch 3/4: 0.191s\n",
      "[PROFILE] Inference batch 4/4: 0.163s\n",
      "[PROFILE] Inference batch 1/4: 0.156s\n",
      "[PROFILE] Inference batch 2/4: 0.165s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.154s\n",
      "[PROFILE] Inference batch 1/4: 0.175s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.152s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.180s\n",
      "[PROFILE] Inference batch 2/4: 0.170s\n",
      "[PROFILE] Inference batch 3/4: 0.175s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.131s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.133s\n",
      "[PROFILE] Inference batch 1/4: 0.161s\n",
      "[PROFILE] Inference batch 2/4: 0.146s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.126s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.157s\n",
      "[PROFILE] Inference batch 4/4: 0.118s\n",
      "[PROFILE] Inference batch 1/4: 0.195s\n",
      "[PROFILE] Inference batch 2/4: 0.188s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.167s\n",
      "[PROFILE] Inference batch 1/4: 0.203s\n",
      "[PROFILE] Inference batch 2/4: 0.141s\n",
      "[PROFILE] Inference batch 3/4: 0.172s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 2.176s\n",
      "[PROFILE] Inference batch 2/4: 0.052s\n",
      "[PROFILE] Inference batch 3/4: 0.057s\n",
      "[PROFILE] Inference batch 4/4: 0.042s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_eb4fd4', 'Motor axis 0': 70, 'Motor axis 1': 780, 'Motor axis 2': 33}\n",
      "Motor found in d:/flagellar/data//train/tomo_eb4fd4 at position: z=70, y=780, x=33\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_dc9a96 (1/1)\n",
      "Processing 500 out of 500 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.493s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.169s\n",
      "[PROFILE] Inference batch 4/4: 0.173s\n",
      "[PROFILE] Inference batch 1/4: 0.222s\n",
      "[PROFILE] Inference batch 2/4: 0.174s\n",
      "[PROFILE] Inference batch 3/4: 0.209s\n",
      "[PROFILE] Inference batch 4/4: 0.176s\n",
      "[PROFILE] Inference batch 1/4: 0.200s\n",
      "[PROFILE] Inference batch 2/4: 0.190s\n",
      "[PROFILE] Inference batch 3/4: 0.218s\n",
      "[PROFILE] Inference batch 4/4: 0.165s\n",
      "[PROFILE] Inference batch 1/4: 0.229s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.223s\n",
      "[PROFILE] Inference batch 4/4: 0.211s\n",
      "[PROFILE] Inference batch 1/4: 0.236s\n",
      "[PROFILE] Inference batch 2/4: 0.220s\n",
      "[PROFILE] Inference batch 3/4: 0.217s\n",
      "[PROFILE] Inference batch 4/4: 0.220s\n",
      "[PROFILE] Inference batch 1/4: 0.213s\n",
      "[PROFILE] Inference batch 2/4: 0.261s\n",
      "[PROFILE] Inference batch 3/4: 0.247s\n",
      "[PROFILE] Inference batch 4/4: 0.226s\n",
      "[PROFILE] Inference batch 1/4: 0.257s\n",
      "[PROFILE] Inference batch 2/4: 0.258s\n",
      "[PROFILE] Inference batch 3/4: 0.240s\n",
      "[PROFILE] Inference batch 4/4: 0.228s\n",
      "[PROFILE] Inference batch 1/4: 0.257s\n",
      "[PROFILE] Inference batch 2/4: 0.252s\n",
      "[PROFILE] Inference batch 3/4: 0.241s\n",
      "[PROFILE] Inference batch 4/4: 0.242s\n",
      "[PROFILE] Inference batch 1/4: 0.256s\n",
      "[PROFILE] Inference batch 2/4: 0.257s\n",
      "[PROFILE] Inference batch 3/4: 0.234s\n",
      "[PROFILE] Inference batch 4/4: 0.238s\n",
      "[PROFILE] Inference batch 1/4: 0.245s\n",
      "[PROFILE] Inference batch 2/4: 0.243s\n",
      "[PROFILE] Inference batch 3/4: 0.235s\n",
      "[PROFILE] Inference batch 4/4: 0.227s\n",
      "[PROFILE] Inference batch 1/4: 0.254s\n",
      "[PROFILE] Inference batch 2/4: 0.249s\n",
      "[PROFILE] Inference batch 3/4: 0.183s\n",
      "[PROFILE] Inference batch 4/4: 0.240s\n",
      "[PROFILE] Inference batch 1/4: 0.234s\n",
      "[PROFILE] Inference batch 2/4: 0.288s\n",
      "[PROFILE] Inference batch 3/4: 0.231s\n",
      "[PROFILE] Inference batch 4/4: 0.234s\n",
      "[PROFILE] Inference batch 1/4: 0.250s\n",
      "[PROFILE] Inference batch 2/4: 0.248s\n",
      "[PROFILE] Inference batch 3/4: 0.198s\n",
      "[PROFILE] Inference batch 4/4: 0.233s\n",
      "[PROFILE] Inference batch 1/4: 0.252s\n",
      "[PROFILE] Inference batch 2/4: 0.254s\n",
      "[PROFILE] Inference batch 3/4: 0.203s\n",
      "[PROFILE] Inference batch 4/4: 0.233s\n",
      "[PROFILE] Inference batch 1/4: 0.268s\n",
      "[PROFILE] Inference batch 2/4: 0.243s\n",
      "[PROFILE] Inference batch 3/4: 0.220s\n",
      "[PROFILE] Inference batch 4/4: 0.233s\n",
      "[PROFILE] Inference batch 1/4: 2.330s\n",
      "[PROFILE] Inference batch 2/4: 0.130s\n",
      "[PROFILE] Inference batch 3/4: 0.079s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "{'tomo_id': 'd:/flagellar/data//train/tomo_dc9a96', 'Motor axis 0': -1, 'Motor axis 1': -1, 'Motor axis 2': -1}\n",
      "Motor found in d:/flagellar/data//train/tomo_dc9a96 at position: z=-1, y=-1, x=-1\n",
      "Current detection rate: 1/1 (100.0%)\n",
      "Using GPU: NVIDIA GeForce RTX 4070 Ti with 12.88 GB memory\n",
      "Dynamic batch size set to 32 based on 12.63GB free memory\n",
      "Processing tomogram d:/flagellar/data//train/tomo_9cde9d (1/1)\n",
      "Processing 600 out of 600 slices (CONCENTRATION=1)\n",
      "[PROFILE] Inference batch 1/4: 4.562s\n",
      "[PROFILE] Inference batch 2/4: 0.167s\n",
      "[PROFILE] Inference batch 3/4: 0.177s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.154s\n",
      "[PROFILE] Inference batch 3/4: 0.155s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.163s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.162s\n",
      "[PROFILE] Inference batch 4/4: 0.122s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.183s\n",
      "[PROFILE] Inference batch 3/4: 0.180s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.157s\n",
      "[PROFILE] Inference batch 3/4: 0.173s\n",
      "[PROFILE] Inference batch 4/4: 0.107s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.164s\n",
      "[PROFILE] Inference batch 4/4: 0.151s\n",
      "[PROFILE] Inference batch 1/4: 0.179s\n",
      "[PROFILE] Inference batch 2/4: 0.176s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.115s\n",
      "[PROFILE] Inference batch 1/4: 0.172s\n",
      "[PROFILE] Inference batch 2/4: 0.179s\n",
      "[PROFILE] Inference batch 3/4: 0.187s\n",
      "[PROFILE] Inference batch 4/4: 0.131s\n",
      "[PROFILE] Inference batch 1/4: 0.158s\n",
      "[PROFILE] Inference batch 2/4: 0.112s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.127s\n",
      "[PROFILE] Inference batch 1/4: 0.174s\n",
      "[PROFILE] Inference batch 2/4: 0.171s\n",
      "[PROFILE] Inference batch 3/4: 0.161s\n",
      "[PROFILE] Inference batch 4/4: 0.103s\n",
      "[PROFILE] Inference batch 1/4: 0.129s\n",
      "[PROFILE] Inference batch 2/4: 0.168s\n",
      "[PROFILE] Inference batch 3/4: 0.156s\n",
      "[PROFILE] Inference batch 4/4: 0.136s\n",
      "[PROFILE] Inference batch 1/4: 0.165s\n",
      "[PROFILE] Inference batch 2/4: 0.166s\n",
      "[PROFILE] Inference batch 3/4: 0.170s\n",
      "[PROFILE] Inference batch 4/4: 0.153s\n",
      "[PROFILE] Inference batch 1/4: 0.168s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.145s\n",
      "[PROFILE] Inference batch 1/4: 0.170s\n",
      "[PROFILE] Inference batch 2/4: 0.151s\n",
      "[PROFILE] Inference batch 3/4: 0.160s\n",
      "[PROFILE] Inference batch 4/4: 0.139s\n",
      "[PROFILE] Inference batch 1/4: 0.178s\n",
      "[PROFILE] Inference batch 2/4: 0.137s\n",
      "[PROFILE] Inference batch 3/4: 0.158s\n",
      "[PROFILE] Inference batch 4/4: 0.166s\n",
      "[PROFILE] Inference batch 1/4: 0.177s\n",
      "[PROFILE] Inference batch 2/4: 0.182s\n",
      "[PROFILE] Inference batch 3/4: 0.185s\n",
      "[PROFILE] Inference batch 4/4: 0.158s\n",
      "[PROFILE] Inference batch 1/4: 0.173s\n",
      "[PROFILE] Inference batch 2/4: 0.184s\n",
      "[PROFILE] Inference batch 3/4: 0.168s\n",
      "[PROFILE] Inference batch 4/4: 0.143s\n",
      "[PROFILE] Inference batch 1/4: 0.171s\n",
      "[PROFILE] Inference batch 2/4: 0.177s\n",
      "[PROFILE] Inference batch 3/4: 0.153s\n",
      "[PROFILE] Inference batch 4/4: 0.161s\n"
     ]
    }
   ],
   "source": [
    "if not fls.is_submission:\n",
    "    importlib.reload(flg_yolo)\n",
    "    model = fls.dill_load(fls.temp_dir + \"temp.pickle\")\n",
    "    inferred_test_data = model.infer(test_data)\n",
    "    print(fls.score_competition_metric(inferred_test_data, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b1cac7-f4ce-45fc-9669-8997b6865bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_test_data2 = model.infer(fls.load_all_test_data())\n",
    "fls.write_submission_file(inferred_test_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5ee85-7069-4a09-a08c-2492e410632b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_test_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a7c36-dd8c-4454-ac52-6f5762af3a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11294684,
     "isSourceIdPinned": false,
     "sourceId": 91249,
     "sourceType": "competition"
    },
    {
     "datasetId": 6925042,
     "sourceId": 11204341,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6949538,
     "sourceId": 11204343,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 211097053,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 229283084,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
