{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0b4b630-bd17-4d7f-97ef-001707dcf970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MainProcess\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7e91596ddc90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('d:/flagellar/code/core')\n",
    "sys.path.append('/flagellar/code/core')\n",
    "sys.path.append('/flagellar/input/my-flg-library/')\n",
    "import flg_support as fls\n",
    "import importlib\n",
    "import numpy as np\n",
    "import flg_diagnostics\n",
    "import flg_numerics\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import copy\n",
    "import flg_preprocess\n",
    "import os\n",
    "fls.profiling=False\n",
    "plt.scatter([0,1],[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758650f7-df3d-4e50-857a-b8d2c213fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(fls.result_dir + '/many_abbr_full/Baseline_0M*')\n",
    "models = [fls.dill_load(f) for f in files]\n",
    "models = sorted(models, key=lambda d:d.trained_model.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8215a379-7b4b-4784-8624-20fb136995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo = slice(None)\n",
    "data = copy.deepcopy(models[0].inferred_test_data[todo])\n",
    "train_data = fls.load_all_train_data()+fls.load_all_test_data()\n",
    "train_data_new = []\n",
    "for t in models[0].train_data:\n",
    "    for d in train_data:\n",
    "        if t.name == d.name:\n",
    "            train_data_new.append(d)\n",
    "models[0].train_data = train_data_new  \n",
    "test_data_new = []\n",
    "for t in models[0].test_data:\n",
    "    for d in train_data:\n",
    "        if t.name == d.name:\n",
    "            test_data_new.append(d)\n",
    "models[0].test_data = test_data_new  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01866c8d-120a-4a62-b9aa-b3c1d0ee2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for m in models[0:1]:\n",
    "#     m.trained_model.step1Labels.relative_confidence_threshold = 0.001\n",
    "#     m.inferred_test_data = m.trained_model.infer(m.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4be95a9-3dda-4329-808f-2af9e030e6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[0].trained_model.step1Labels.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e6427a-95b0-4b6a-8b6c-0e69abae7be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!\n",
      "CompletedProcess(args=['pip', 'uninstall', '-y', 'albumentations'], returncode=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping albumentations as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6344c4967aa4542bc25c7037d79c37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/cupyx/jit/_interface.py:173: FutureWarning: cupyx.jit.rawkernel is experimental. The interface can change in the future.\n",
      "  cupy._util.experimental('cupyx.jit.rawkernel')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471216c7f6494be090747df9cd6608a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Complete:\n",
      "- Training data: 575 tomograms, 0 motors, 0 slices\n",
      "- Validation data: 72 tomograms, 0 motors, 0 slices\n",
      "- Dataset directory: /flagellar/temp//yolo_dataset/\n",
      "- YAML configuration: /flagellar/temp//yolo_dataset/dataset.yaml\n",
      "\n",
      "Ready for YOLO training!\n",
      "Starting YOLO training process...\n",
      "Created new YAML at /flagellar/temp/training.yaml\n",
      "Using YAML file: /flagellar/temp/training.yaml\n",
      "YAML contents:\n",
      "names:\n",
      "  0: motor\n",
      "path: /flagellar/temp//yolo_dataset/\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n",
      "\n",
      "Starting YOLO training...\n",
      "New https://pypi.org/project/ultralytics/8.3.127 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "WARNING âš ï¸ 'crop_fraction' is deprecated and will be removed in in the future.\n",
      "Ultralytics 8.3.126 ðŸš€ Python-3.11.10 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4090, 24210MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=None, batch=12, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/flagellar/temp/training.yaml, degrees=0.0, deterministic=True, device=cuda:0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.2, mode=train, model=yolov9s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=motor_detector, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/flagellar/temp//yolo_weights/, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/flagellar/temp/yolo_weights/motor_detector, save_frames=False, save_json=False, save_period=5, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     31104  ultralytics.nn.modules.block.ELAN1           [64, 64, 64, 32]              \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.block.AConv           [64, 128]                     \n",
      "  4                  -1  1    258432  ultralytics.nn.modules.block.RepNCSPELAN4    [128, 128, 128, 64, 3]        \n",
      "  5                  -1  1    221568  ultralytics.nn.modules.block.AConv           [128, 192]                    \n",
      "  6                  -1  1    579648  ultralytics.nn.modules.block.RepNCSPELAN4    [192, 192, 192, 96, 3]        \n",
      "  7                  -1  1    442880  ultralytics.nn.modules.block.AConv           [192, 256]                    \n",
      "  8                  -1  1   1028864  ultralytics.nn.modules.block.RepNCSPELAN4    [256, 256, 256, 128, 3]       \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPELAN         [256, 256, 128]               \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    628800  ultralytics.nn.modules.block.RepNCSPELAN4    [448, 192, 192, 96, 3]        \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    283008  ultralytics.nn.modules.block.RepNCSPELAN4    [320, 128, 128, 64, 3]        \n",
      " 16                  -1  1    110784  ultralytics.nn.modules.block.AConv           [128, 96]                     \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    598080  ultralytics.nn.modules.block.RepNCSPELAN4    [288, 192, 192, 96, 3]        \n",
      " 19                  -1  1    221440  ultralytics.nn.modules.block.AConv           [192, 128]                    \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1061632  ultralytics.nn.modules.block.RepNCSPELAN4    [384, 256, 256, 128, 3]       \n",
      " 22        [15, 18, 21]  1   1563475  ultralytics.nn.modules.head.Detect           [1, [128, 192, 256]]          \n",
      "YOLOv9s summary: 544 layers, 7,287,795 parameters, 7,287,779 gradients, 27.4 GFLOPs\n",
      "\n",
      "Transferred 1333/1339 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2460.8Â±998.4 MB/s, size: 122.4 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /flagellar/temp/yolo_dataset/labels/train... 655 images, 335 backgrounds, 2 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 655/655 [00:00<00:00, 2909.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0m/flagellar/temp/yolo_dataset/images/train/tomo_a5ac23_z0169.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0908]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/flagellar/temp/yolo_dataset/images/train/tomo_a5ac23_z0171.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0908]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /flagellar/temp/yolo_dataset/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1520.4Â±922.0 MB/s, size: 110.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /flagellar/temp/yolo_dataset/labels/val... 71 images, 29 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:00<00:00, 2674.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /flagellar/temp/yolo_dataset/labels/val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /flagellar/temp/yolo_weights/motor_detector/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 221 weight(decay=0.0), 228 weight(decay=0.00046875), 227 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/flagellar/temp/yolo_weights/motor_detector\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      3.83G       4.07      64.19      1.714          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:06<00:00,  8.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         71         42   0.000772       0.19   0.000659   0.000286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      4.17G       3.37      4.647       1.27          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [00:05<00:00, 10.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 18.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         71         42     0.0913     0.0238     0.0392    0.00968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      4.17G      3.022      3.552        1.2          8        640:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 42/55 [00:03<00:01, 11.33it/s]"
     ]
    }
   ],
   "source": [
    "highest_false_score = []\n",
    "real_score = []\n",
    "data_list = []\n",
    "for i_type in [3]:#range(4):\n",
    "    model = copy.deepcopy(models[0].untrained_model)\n",
    "    model.step1Labels.relative_confidence_threshold = 0.01\n",
    "    #model.step1Labels.n_epochs = 2\n",
    "    #m.step2Motors.distance_threshold = 10.\n",
    "    if i_type == 0:\n",
    "        # baseline\n",
    "        pass\n",
    "    elif i_type==1:\n",
    "        # blur\n",
    "        model.step1Labels.preprocessor = flg_preprocess.Preprocessor2()\n",
    "    elif i_type==2:\n",
    "        # no resize\n",
    "        model.step1Labels.preprocessor = flg_preprocess.Preprocessor2()\n",
    "        model.step1Labels.preprocessor.target_voxel_spacing = 20.\n",
    "        model.step1Labels.box_size = 18\n",
    "        model.step1Labels.prevent_ultralytics_resize = True        \n",
    "    elif i_type==3:\n",
    "        # negative labels\n",
    "        model.step1Labels.preprocessor = flg_preprocess.Preprocessor2()\n",
    "        model.step1Labels.preprocessor.target_voxel_spacing = 20.\n",
    "        model.step1Labels.box_size = 18\n",
    "        model.step1Labels.prevent_ultralytics_resize = True     \n",
    "        model.step1Labels.negative_label_threshold = 0.6\n",
    "        model.step1Labels.negative_slice_ratio = 0.        \n",
    "    model.run_in_parallel = False   \n",
    "    model_file = fls.temp_dir + 'model_' + str(i_type) + '.pickle'\n",
    "    if not os.path.isfile(model_file):\n",
    "        print('!!!')\n",
    "        model.step1Labels.trust = 0\n",
    "        model.train(models[0].train_data, models[0].test_data)\n",
    "        fls.dill_save(model_file, model)\n",
    "    model = fls.dill_load(model_file)\n",
    "    #model = fls.dill_load(fls.temp_dir + 'model_' + str(i_type) + '.pickle')\n",
    "    data_file = fls.temp_dir + 'data_' + str(i_type) + '.pickle'\n",
    "    if not os.path.isfile(data_file):        \n",
    "        inferred_data = model.infer(models[0].test_data[todo])\n",
    "        fls.dill_save(data_file, inferred_data)\n",
    "    inferred_data = fls.dill_load(data_file)    \n",
    "    for d in inferred_data:\n",
    "        d.labels_unfiltered = d.labels_unfiltered2\n",
    "    fls.mark_tf_pn(inferred_data, models[0].test_data[todo])\n",
    "    data_list.append(inferred_data)\n",
    "    this_highest_false_score = []\n",
    "    this_real_score = []\n",
    "    for i,r in zip(inferred_data, models[0].test_data[todo]):\n",
    "        false_positives = i.labels_unfiltered[i.labels_unfiltered['tf_pn']==1.]\n",
    "        if len(false_positives)>0:\n",
    "            this_highest_false_score.append(np.max(false_positives['confidence']))\n",
    "        else:\n",
    "            this_highest_false_score.append(0)\n",
    "        if len(r.labels)>0:\n",
    "            true_positives = i.labels_unfiltered[i.labels_unfiltered['tf_pn']==0.]\n",
    "            if len(true_positives)>0:\n",
    "                this_real_score.append(np.max(true_positives['confidence']))\n",
    "            else:\n",
    "                this_real_score.append(0)\n",
    "    highest_false_score.append(this_highest_false_score)\n",
    "    real_score.append(this_real_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc0b71-e6bf-4be0-be84-f1b795f5f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_list:\n",
    "    n_corr=0\n",
    "    n_total=0\n",
    "    for i,r in zip(d, models[0].test_data[todo]):\n",
    "        if len(r.labels)>0:# and 'tom' in r.name:\n",
    "            n_total+=1\n",
    "            ind = np.argmax(i.labels_unfiltered['confidence'])\n",
    "            if (i.labels_unfiltered['tf_pn'].tolist()[ind])==0.:\n",
    "                n_corr += 1\n",
    "    print(n_corr/n_total)\n",
    "    #print(i.labels_unfiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66521e7b-0060-4b98-855e-534a616b6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_baseline = 2; i_new = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586a6b9-0f48-42c1-91a0-f3bbdb3b745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(highest_false_score[i_baseline], highest_false_score[i_new])\n",
    "plt.xlabel('Highest false score baseline')\n",
    "plt.ylabel('Highest false score new')\n",
    "plt.grid(True)\n",
    "plt.axline((0,0),slope=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f443a8-8e9a-46a9-a02e-489c4723f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(real_score[i_baseline], real_score[i_new])\n",
    "plt.xlabel('True score baseline')\n",
    "plt.ylabel('True score new')\n",
    "plt.grid(True)\n",
    "plt.axline((0,0),slope=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad4403f-94e4-4756-aecc-d50d9e7ae68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=(np.argwhere(np.logical_and(np.array(real_score[i_baseline])>0.6, np.array(real_score[i_new])<0.1))).flatten()\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d867d-d02c-453d-a7da-1dbb95971635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cur_id = 87\n",
    "cur_id = 0\n",
    "for ii in range(len(models[0].test_data[todo])):\n",
    "    if len(models[0].test_data[todo][ii].labels)>0:     \n",
    "        if cur_id in xx:\n",
    "            print(models[0].test_data[todo][ii].name,ii,real_score[i_baseline][cur_id],real_score[i_new][cur_id])\n",
    "        cur_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e485affd-23ba-48f5-9f15-60523b8f0754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# flatten the two arrays into one to get global bin edges\n",
    "all_scores = np.concatenate([highest_false_score[i_baseline], highest_false_score[i_new]])\n",
    "# choose number of bins (e.g. 30) or compute automatically\n",
    "bins = np.histogram_bin_edges(all_scores, bins=30)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(highest_false_score[i_baseline], bins=bins, cumulative=True, alpha=0.5, label='Original')\n",
    "plt.hist(highest_false_score[i_new], bins=bins, cumulative=True, alpha=0.5, label='New')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Overlayed Histograms of Highest False Scores - lower is better')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f197e-1e29-4fcc-acd7-51519c970130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# flatten the two arrays into one to get global bin edges\n",
    "all_scores = np.concatenate([real_score[i_baseline], real_score[i_new]])\n",
    "# choose number of bins (e.g. 30) or compute automatically\n",
    "bins = np.histogram_bin_edges(all_scores, bins=30)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(real_score[i_baseline], bins=bins, cumulative=True, alpha=0.5, label='Original')\n",
    "plt.hist(real_score[i_new], bins=bins, cumulative=True, alpha=0.5, label='New')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Overlayed Histograms of True Scores - higher is better')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46917a-1b76-4815-bb1f-54901e1aaec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a4abd2-2f3a-4bc6-adb0-b83e6b360aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
